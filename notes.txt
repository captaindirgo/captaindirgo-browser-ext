What about p2p model?

Each node on the service would have a private/public key pair

Messages would be sent back and forth. Messages such as:
get_comments -- given a url get comments
get_username -- get a username for a pubkey

each public key has a txn chain regarding operations on it.
such as:
post_comment
upvote
downvote
trust -- trust other user
untrust
each url would be a set of users that posted to it.

2022/09/04

Today, I'm fasting so I'm making an earlier log entry before I lose all my mental energy.

I've been looking at p2p lately. I thought about it, and using a centralized p2p forum to power the thing seems to be going down a rabbithole. First because of what I've seen the stuff that's out there is very bloated, poorly documented, and hard to customize, and since I'm a beginner it makes it even more so. Besides, even if I were to succeed, then so what? All I'd get out of it is a good prototype, but I'd still have to move to somethig more decentralized.

So, I'm shifting gears and going p2p again. Maybe it's obvious, but I realized when trying to understand what a library does, it's best to look at its dependencies first, so if I want to understand hypercore, I look and see it depends on hyperstream and secret-stream, and then look to see that hyperstream depends on hyperstream/dht, etc. Then I just look at dht and secretstream and then go upwards from there.

So, from what I found, there is no way to create a p2p network without any fixed addresses. Even magnet links have bootstrapping nodes hardcoded in the client. So what I plan to do is to piggyback on bittorrent's magnet protocol. That way, in order to knock out captain dirgo through DHT, you'd have to knock out the whole bittorrent infrastructure.

The big elephant in the room then is spam. There are a couple of projects I want to look at that are also based on hypercore that would have to deal with spam, that I want to look at to get ideas. One is https://github.com/Telios-org which is a p2p email service, and the other is https://cabal.chat/ which is a p2p chat network. I found these here: https://dat-ecosystem.org/

My own idea is to use something like hashcash, so that to post a message, you'd have to mine some data akin to a bitcoin miner. This would be as light as possible, for a url with zero comments, or few views, maybe mining wouldn't be needed at all, but one with a lot of comments, you would have to mine for awhile. Also along with that, having trust networks, so that if your user is trusted by another node on the network it could post without mining.  That other node would have rules, such as having to solve a captcha to initialize the node, and other rules like not posting too quickly.

In the end, its up to each individual user about what messages they want to see or not, so when using the extension you could personally dial in how much spam you are willing to tolerate.

2022/09/05

Didn't have a lot of time today.

Read about Mainline DHT. I think it will work nicely for Captain Dirgo, just present it like a regular torrent with the name "captaindirgo". But when clients connect, they would use captain dirgo's protocol rather than bittorrent.

Started working on adding webtorrent/dht to the extension. This will allow the extension to bootstrap from the regular magnet network that bittorrent uses. Should be pretty hard to shut down.

Played around with web-ext so that I can more easily write a extension working in both chromium and firefox.

2022/9/7

Worked on understanding webpack. I know I am dicking around with a lot of tools, but in the end I think the increased productivity will be worth it.

I understand it pretty well, and it beats everything else that I've seen. It works kind of like java compilation, in that when you require one file from another, it will automatically build the dependency.

Also looked into using lbry as a backend. I'm somewhat skeptical of it for my use case, since it seems to be built around serving static files (ie videos and images). Since captain dirgo is going to be dynamic in that comments are added whenever anyone wants to, I don't think it would work well.

I was thinking about the project last night. Below are some notes, but with the caveat that these are only ideas and I'm not committed to implementing them:

 - I think it should have  a blockchain, so there is a consistent state between all peers. But I don't think it needs mining or a coin. It will have lightweight clients and heavyweight clients, and servers (also heavy weights)
 - Heavyweights can read and provide any part of the blockchain
 - Merkle tree backing 
 - Servers compute final state. (This comment voted up/down, blocked, etc.) And can be queried for state.
 - Users can choose to trust the servers they want.
 - So captaindirgo.com would be a server.
 - Hopefully other people will want to be servers. Websites can also be servers. Benefits would be to draw traffic to their website. This would make captaindirgo harder to kill, because you'd have to kill all the servers. (Also a server doesn't have to have a website, it could be spun up by anybody)
 - Servers could also vouch for users, using email verification, captchas, etc. If a user trusts a particular server, comments vouched by that server would float to the top. This way servers can eliminate spam, but not in a way where they are able to censor the network.
 - Servers could also validate keys with user names. So, a user could have a specific icon next to it indicating that a server has vouched for it (much like a twitter blue check, but each server has their own icon and users could choose which ones they trust).
 - Blocked messages can be stored as just their hash, so that an illegal message, such as a dox or call for violence, can be removed from the blockchain by a particular server (this won't prevent other servers from keeping the data, however).

Status Update 2022/9/8

I just today realized that DHT will not work in the browser, feel kind of stupid about it.

Webtorrent, however, is somehow resolving magnet links within the browser, so I need to
parse the source code to figure out it does so. This way, hopefully I can piggyback on
another service to have a more decentralized way of finding captaindirgo servers.

I saw this pretty good discussion on persistent service workers:

https://stackoverflow.com/questions/66618136/persistent-service-worker-in-chrome-extension

With the new service-worker model that chrome has and mozilla is moving to, I don't think
I can create a persistent peer for a hyperswarm network within the extension without
resorting to some hacks. So I wonder what is the point of a p2p network like that.

Other than looking into this p2p stuff, I started setting up a webpack framework for 
captaindirgo-extension.

2022/9/10

While searching for examples, came across a template for browser extensions. It's the only one that I tried that worked with no errors, and has all the features I want. Hopefully it will save me a lot of time: https://github.com/davidnguyen179/web-extension-svelte-boilerplate

I was able to get it compile out of the box and run in both chromium and firefox.

It's using a framework called svelte which seems pretty cool. I took a look at it and it's a lot easier to use than react and vue and should be faster and less bloated because it generates javascript at compile time rather than runtime like react/vue does.

So today, I went through the tutorial on svelte, and I think this is what I'll use for the extension frontend if nothing goes wrong. I think by learning this framework will be the fastest way for me to create a prototype.

I again didn't get to work too much again, unfortunately, due to having to do some errands around the house.

I was thinking about adding the ability to know how many comments were posted for a website even before clicking on the extension, by making the total show up in the extension icon.

One problem is that for privacy purposes, you don't want the extension looking up every url the user goes to against the server. So, only when the click the extension button do you want the client to ask the server about an individual url.

In order to solve this, I came up with an idea. First the extensio would hash the url using a secure hash algorithm, such as SHA-256, then send, lets say, the first 2 bytes of the hash to the server. This would reduce the number of possible urls that match the hash on average to 1/(256*256) = 1/65536. Then, if the resulting list is small enough, the server could just send all the matching urls along with a total comment count for each. If it's not small enough, then that server would just ask for another byte. By doing it this way, there is no way to determine what url the user went to with only 2 bytes of a sha-256 hash. I mean, literally the client would be sending a string such as "3f" to the server, and there is no way to guess what url the user went to from this. I was also thinking of how to integrate with websites that want to sign up easily. Using the plugin, I can open up any url, including the cookies that are active. So all the website would have to do put a page in the logged-in area of the website that signs the brower-extensions pub key for the user, along with the username and maybe an icon or something. If the user is not logged in, the extension can redirect them to a login page, or open it in a popup window or something and try again afterwards. Then the backend servers for captaindirgo would be able to verify that the user had signed into the site.

Although, I need to check if there are more standard ways to do this, because if there are prebuilt plugins that work with the website, it would make the website owner and my life a whole lot easier.

For the backend servers, I think I want to make them completely separate from the frontend servers. So if you run a website, you can allow your userbase to use the captain dirgo plugin by only building out the signing page. The storage of comments would happen automatically. And if you want to help out by running a backend server, you can do that, too, by running a completely different application, that connects to a p2p network for the captain dirgo backend.

Additionally, anyone could run a backend application without running a website if they wanted to, too.

But that's just my thinking at the moment and could change. My first priority is to get a prototype done, so I can get some feedback on the UI.

2022/9/11

Didn't get much sleep last night. Felt a little off because of it I so did some grunt work. I came up with an initial logo, pretty basic, but it'll do for now.
I also setup gpg for email and initial github repository and updated the README. Here it is: https://github.com/captaindirgo/captaindirgo-browser-ext

From now on I'm going to post status updates there as well as here, in order to increase visibility for the extension and hopefully get other people that want to help to notice.

Went through some more of the svelte tutorial. I think I'll wait until tomorrow or even later before I try to start working on a design/implementing the prototype proper.

I was thinking about the design some more. By default all urls that are sent from the extension should be securely hashed, without the original url ever being known by the server. This will again help user privacy, since even if you have access to the server, you can only tell the website users were commenting on if you happen to guess the url, or brute force it. In addition, I can even use the url with a salt appended to encrypt the comments. In this way, in addition to not knowing what the url is, you couldn't even read the comments unless you knew the url beforehand.

The extension will also always remove all the data after the '?' in a url before hashing it and sending it to a server. Anything after the '?' is where all the personal info is supposed to go. But incase a website developer is stupid enough to design their urls like http://foo.com/<userid>/<account>/<password>/<ssn>/<... other personal details, etc. ...>/, then because of the above, regardless if the user comments on it, the url won't be visible to anyone else including the server owner. 

The above is something the original gab dissenter plugin didn't do, BTW. Anytime a user commented the original dissenter sent everything before the '?' in the url to the gab owned server. I guess they did that so they could maintain a list of 'hot' urls where everyone was commenting to show everyone, like the "trending" data on twitter. I could probably do something like that by having a threshold where if, for example, over 50 users commented on a url, then the next time a user commented on it, it would be released in plaintext to the server. But it'd be kind of messy to implement, and there is a security hole because the server could just lie and tell the extension that 50 users commented on it, so they would release the plain text.

I thought of other idea to help prevent spam by a great deal but I'm too tired today so I think wouldn't be able to explain it well, so I will wait until tomorrow.

Thanks for reading.

2022/9/12

Today I spent the majority of my time going through some more of the svelte tutorial. I started at chapter 5 and now am through 10. There are 19 chapters in total, and I think after I've gone through them all I should be able to start hammering out a gui.

I also want to come up with a handwritten mockup of the components, which I will try to work on tomorrow, not sure when I'll finish. I'll upload it here when it's done.

I realized my idea that I didn't explain yesterday to prevent spam wouldn't work, so I won't hash it out here.

2022/9/13

Finished svelete tutorial. I found a basic comments widget in svelte here: https://github.com/loganwoolf/svelte-comments It's a little basic, being developed to win a contest, but that should make it easier to read and modify.

I was thinking of trying to draw out a design, and other documents yesterday, but I don't know if I really need any of this to get started, as long as I have it laid out in my mind.

So I'm just going to get started and start coding and experimenting and iterate from there.

Learned about https://msgpack.org/index.html which I might adopt for communication between nodes.

Started work on getting an options page up for the extension. Figured that will be a good starting point because everything else is going to need to use it to get user preferences.

Learned about jest, which is a testing framework and was already part of the template that I'm using. Will try to make good use of it.

2022/9/14

I learned some UI stuff, flexblox, browser storage, svelte storage.

I wanted to work on an options page for the extension, but I didn't really want to write a page of fake options since I wasn't sure what I needed, so I decided to delve more into the backend so I had an idea of what I would need.

I read the bittorrent spec and the hypercore whitepaper. Strangely bittorrent seemed more
advanced as far as preventing dos, even though its much older. It has choking to prevent peers from asking for too much data too quickly, and a draft proposal called "Canonical Peer Priority" which helps spread out the node connections. hypercore has built in encryption, but I didn't find it so useful, since everything is going to be encrypted anyway.

I guess bittorrent has been around a long time and therefore had more time to workout all the flaws.

I also looked more into browser based implementations of hypercore and bittorent. There was RangerMauve's implementation of hypercore, which just seems to route all data through a proxy, as far as I could tell reading the source code. The documentation wasn't very clear. This would mean it's basically useless as a p2p network.

I also came across https://github.com/draeder/easypeers which is short little 200 line project that establishes a webtorrent webrtc network. I was able to get it to work both in the browser and nodejs so it seems like a good place to start.

So, I'm leaning towards the webtorrent protocol. I think I'm going to separate the data out per url. This way, anyone who runs the extension can help the network out, and have a cache of comments for recently visited sites, by storing the comments and other data associated to the urls they visit.

2022/9/15

Here is some thinking about the project that I was doing:

Lets say that the extension derives a private/public keypair based on the url, and the sends the public key to the server.

It then uses the private key to encrypt the comment(if its adding a comment) and sign the entire message that is sent to the network to be stored by server nodes

We can't allow a client to send data willy nilly because they could overwhelm the network with data, and create a ton of messages everywhere, drowning out legitimate conversion.

So we have to have some rules on the server nodes.

We will have a web of trust. So there will be an ultimate trust certificate. It can give trust to other websites, attaching a score from 1 to 0 representing a level of trust. certs can then trust other certs and so on and so on and to determine the final trust level, simply multiple the scores down the path off signed certs.

Then based on this score, the extension can decide whether to display a particular message, the server node can decide whether to accept a message from a peer, etc.

Each user needs to sign into captaindirgo.com. It will have a captcha and a email verifier or something.

Sign in will provide a signed message containing a date, with an expiration and a username.

When users post messages, they provide that signed message along with their message.

Server nodes verify this message. They can then have other rules, like post once a minute or something like that.

---

Today I searched the web for projects to help me get a initial website for signing in working. The idea here is to give the extension some sort of server to start the login process. 

I looked across a lot of different examples. It seemed that every one referenced completely different frameworks and helper libraries from each other. Everything is so fragmented and it's really painful to have to learn another library/framework/web server everytime I want to add something to this project.

I found https://github.com/shinokada/svelte-auth seemed pretty good. It doesn't use a lot of different libraries, frameworks, etc. that I don't know about and the demo is pretty advanced. https://svelte-auth-codewithshin.vercel.app/

It turns out that hyperswarm-web only uses the proxy to bridge webrtc networks and regular peers. Also, https://github.com/geut/discovery-swarm-webrtc has a swarm just for web-rtc which I think will work good for this projects.

I also got started with visual studio code. I was using emacs but I think I should try something more modern.

2022/9/16

I didn't have a lot of time to work today. I looked more into sveltekit. The developers decided to remove a piece of major functionality that everyone was using to define a users session and replace it with something else a couple of weeks ago, but all the examples have yet to be updated.

So I had to try and read and understand this one git discussion thread on this new functionality with no examples to work from. I think I can do some experiments and get it to work, but I don't like the way sveltekit is being developed. It's seems ripe to have bugs or easy to misconfigure and get hacked.

I'm kind of tired now, so I'll think about it tomorrow. I think I need to understand it at a deeper level, I suppose, to really get a good opinion on it. Of course I'd be wasting time understanding this new architecture if I do that and it turns out to be crap.

I thought I was getting a leg up on the whole svelte thing by using it in the server, because I'm pretty sure I'm going to use in the extension code. Also having a nice framework to work from opens possiblities for captaindirgo.com to have more functionality than if I use something simpler. So, we'll see.

2022/9/17

I'm still playing around with svelte. There is a user session manager that is stateless on the server side. It works by having the server encrypt a token containing session information to the client which then returns it back as a cookie. Seemed kind of cool and I thought I might use it simply because it shows some working encryption using nodejs which should help me out when I get to the other pieces. It's here: https://github.com/pixelmund/svelte-kit-cookie-session

I'm trying to get a clearer picture of the separation of client and server within the system. It seems that the main workflow is that for each page the browser loads some preferably static page with javascript and then uses a separate fetch to retrieve dynamic information. At first I thought that was not a great idea, because it involves two round trips, but it makes sense from a certain viewpoint. If you create a page with a nice "loading..." mode and fetch the data afterwards with javascript, it would seem faster to the user, because something gets displayed right away.

I'm not planning on using captaindirgo.com for much, just an overview of the extension, how to download it, and a page to register and log-in (this will be a proof-of-concept, mainly, because I plan on integrating with other websites afterwards, using oauth or something like that) but it seemed like a logical place to start, because the extension will have to point to something.

2022/9/18

I read more about svelte and sveltekit and I think I have a better handle on it. The one thing the documentation is missing, as far as I am concerned, is a flow of how the processing works.

Off the top of my head, there is a hooks.server.js, a hooks.client.js, a layout.server.js, a layout.js, a page.js and a page.svelte file, and I believe that's everything that is run during a request, but I could be wrong.

There is also a server route that gets called for api requests, initiated by the client. There is a pattern I guess, where layout.server.js, and page.js loads any data from the database or whatever is needed on the server side, and places it in a js object to be automatically serialized and sent to the client.

This all happens at when the page loads. Then, afterwards the client can issue fetch requests and get back serialized objects from json.

That's how I understand it to work. They've recently changed a lot and the examples are not up to date and the documentation is terse. But the framework is pretty good, it seems from my first look at it.

I don't think I'm going to waste my time looking at any more outdated examples, and just try to mess around and get a site up. I'm pretty confident I can make good progress from here on out.

2022/9/19

I worked on getting the website up and working. Didn't get too far in it, but got the nav bar working and an initial main page. I still need to do a login, signup and download page.

I looked at some different ways to deploy the website. I want to get a prototype up, so I hopefully can get some feedback on the design.

I was thinking about the extension itself. If Google and Mozilla don't accept it, it is somewhat difficult to install, involving multiple steps, and going into developer mode. And on windows using chrome, it will nag the user everytime they startup to disable any sideloaded extensions.

However, if I make a version of the extension to work using Tampermonkey, that would sidestep all of that. Tampermonkey is an extension that can run arbritrary scripts to alter web pages. So the user would just have to install tampermonkey, and then install a Captain Dirgo script rather than an extension which should be a lot easier.

2022/9/20

Got some more experience with Typescript. Made a couple of interfaces for registration and logging in.

Worked a little on the login page, and I got it to display and basically work.

I also looked into oauth and from first glance it doesn't look like it will work. I'm looking for something where the website can sign an authorization that can be verified out of bounds by the p2p network. As far as I can tell, oauth used a blob token.

Just for curiousity and to try and come up with additional better ideas, I learned about jamstack. I thought about maybe using the p2p network as the backend for the website, but my main concern is to mock up what other websites basically use. This way I can model what I need to do to integrate with them.

2022/9/21

A bit of a short day, I had to make a fence to block the dog from eating compost.


Trying to answer a stupid question. Where can I put a var that represents the db so it is accessible by everyone? Without this I'd have to spin up and shutdown a db connection with each request.

The flagship example sidesteps this by calling an external 'api' url.

I guess that would be connection pooling. But the question remains.

Was able to solve it using https://github.com/sveltejs/kit/issues/1538#issuecomment-848002582

Basically create a promise in hooks.server.ts and that promise will run only once, so it
will function like a global var. Then stick it into events.local and it will be accessible in the rest of the app.

But it turns out that the mongodb nodejs driver already implements a connection pool, so I problably won't need a global variable (for that anyway). Nice to know though.

I think from here on out, most status updates are going to be about pretty trivial things. When the rubber hits the road, it's the silly little things that take the most time, especially when you are getting up to speed with a language.


2022/9/22

From experimentation, it looks like any server side "+page.ts" or similar, only runs once
per application.

Note to self, to use require within sveltekit, run:

    import { createRequire } from 'node:module';
    const require = createRequire(import.meta.url);

    // sibling-module.js is a CommonJS module.
    const fs = require('fs');

I wanted to make a config file in the main directory, but I realized that it won't work there, because sveltekit packages the app before runnig it, so I think it has to go into the src/ directory which is a little weird. 

Looked for authentication libraries for handling user sign in:

https://github.com/Dan6erbond/sk-auth
Example app is broken: https://github.com/Dan6erbond/sk-auth/issues/94
Not sure if the rest of it works, since it's based on an older version of sveltekit

https://github.com/pilcrowOnPaper/lucia-sveltekit
A little complicated to setup.
From https://lucia-sveltekit.vercel.app/getting-started
    Apps using Lucia cannot be deployed to edge functions (CloudFlare Workers, Vercel Edge Functions, Netlify Edge Functions) because it has a dependency on Node's crypto module and other Node native APIs.

Tried netlify to see if it applies to me. Netlify seems to work, but since I'm not using the lucia code (only imported it as a dependency), I'm not sure if it's actually going to.

Got an initial site up, sign up does NOT work. Don't expect much, but if you want to see my progress, it here: https://captaindirgo.com

Source code for the web server is now here and is here: https://github.com/captaindirgo/captaindirgo-webapp/

I was thinking about how the network is going to managed. Although, I don't mind being the sole owner of the entire thing, if the ball really starts rolling and it gets big, I don't think it's good to only have me run it. Therefore, I'm thinking of making a sort of trust system.

I wrote about this before, but the idea is that to start off with, there would be one master, me, who would have so many "trust points", lets say 1 million. Then I could give them to others to generate private keys with. In fact captaindirgo.com would obtain a certain limited number of these points, but not all, maybe 1000

In this way, if captaindirgo.com was seized or taken off line, the master key would still hold a majority of points and I'd be able to create a new site, maybe some other way, using tor, etc. to keep the network up.

In addition, eventually I would hope to get other trustworthy people to hold trust points themselves, so that I could give away enough that I am not the single authority ruling the whole system.

This does, of course, sound a lot like a crypto token, and I will have to research more into different crypto networks to see if I can use one to bootstrap this idea, or maybe tweak my idea in order to make the network as resilient as possible.

2022/9/23

Got dotenv working, however I think it's not good enough, because I would have to manually check if all the env names I want are there. So instead, I plan to use io-ts which should automate the error checking for me. io-ts will also help with the server api as well.

However there is a problem in that sveltekit seems to have no designated place for instance level configuration files. I asked them on github discussions, and someone told me to put it under "src" which I find kind of silly. It definately makes things confusing, because usually all data under "src" is checked in, and since this file contains secrets, it definately should not.

Learned about csrf attacks, and I think I can manage it. Sveltekit automatically checks the origin header, so that should fix it, I made a todo.txt file and put an entry to test this so I won't forget.

Got a little farther with lucia. I think it will work fine with netlify. I don't know if that will be the final way I'm going to deploy, but for now it makes for a good testing ground.

2022/9/25

Sorry, yesterday I did do some work, but I felt exhausted at the end of the day and didn't want to post anything.

I got the lucia example working after some small fixes. I'd like to have applied it as it was to my code but I was following a different example that does things through javascript and not a simple form submit like lucia does. All these variation, and the code is so messy in everything that I look at. I feel like I'm looking at code made by animals. It's like seeing a mouse nest snuggled underneath the hood of an old automobile and the mice have been rewiring things.

I never knew that frontend code had become so messy, each piece eschewing different older standards while holding on other ones.

So anyway, things are slow but progressing. I installed a new library to handle internationalization but I wonder how useful it really is. It separates the web page from the text and then you have these 2+ files to constantly go back and forth from. I kind of think if I just put everything into each page itself, then it would be easier to deal with.

I'm trying to make sure that everything I work on now will give me experience for when I code the plugin, so that I gain a double benefit. 

2022/9/26

I finished signup and added a login page, and although rough, they both work well enough. I now have a capable server that can link up with the extension, so I'm going to shift gears a little bit and work on the extension because that is the most important.

I'm thinking about the architecture and how best to implement it. There seems to be 3 layers of security:

1. At the p2p peer level, a peer can be blocked if it's behaving badly and/or spamming.
2. At the user level, heavyweight peers can refuse to save messages from a user who is spamming data.
3. At the reporting level, heavyweight nodes can remove messages or block users.

As far as #3 goes, this has to be available to prevent doxx attacks. However, I plan to make it so each heavyweight node makes its own decision what to store and what not to. That way a single entity can't block a comment it doesn't like for everyone, but only for users and heavyweight nodes that place enough trust in it.

I was also thinking about using an existing blockchain. Ordinary users who run the extension obviously would not be full nodes on any currently running well known blockchain, because in order to do so, most chains, if not all, require downloading huge gigabytes of data. So the only reason to use one is for the heavyweight nodes on the network. The biggest benefit that I can see is if the network goes down due to an attack, the data will be stored in a well known place that no authority can take down. The end result is that existing messages would be stored and held as long as that blockchain was still running.

I also need to design this web of trust. I have a basic idea of how it should work. Users and heavyweight node operators decide which authorities they trust in the plugin, ex. captaindirgo.com, xyzcorp.com, etc. Then when a message is received, etc. it comes along with a web of trust. captaindirgo trusts site xyzcorp.com with a score of 0.1 and xyzcorp.com trusts user 'foo' with a score of 0.36, so if the user trusts captaindirgo with a score of 1, then the final trust value would be 1 * 0.1 * 0.36 = 0.036.  But I worry about the speed of verifying these signatures, however if there becomes too many. If the user has to verify thousands of signatures to read the comments on a url, it could get very cpu intensive.

2022/9/27

I looked back at hyperswarm-web, discovery-web, etc. but wasn't really getting it. So, I decided to investigate more about WebRTC. I thought it was just some protocol that opened tcp sockets and had a library tacked on for video and audio, etc. But I was very wrong.

First, it uses UDP and second it has a lot of functionality to burrow through firewalls, nat routers, and other restricted network environments. It seems to me that it would make it possible for anyone to run a full node on the captaindirgo network by simply opting-in through the browser extension, or using a simple app, without needing to know how to open up a port in the firewall or anything about uPNP, etc. In otherwords, it would just work, regardless of the network environment (although I'm sure there will be exceptions).

I'm basing all this information on this 1hr 10min video I watched today about it. It's actually a great video, you can see it here: https://yewtu.be/watch?v=8I2axE6j204

So, as far as I understand it, webrtc works using these things called SDP's which are similar to ipaddress and ports. SDP's can be enhanced by contacting an outside public server and receiving back an ICE message. This allows a client to know it's outside ip and faciliate p2p connections even when both clients are behind firewalls, nats, etc. without any user configuration.

There is also a drawback to webrtc in that, suppose client A uses it and gets its own SDP. It still needs to establish some sort of connection to client B without using webrtc in order for client B to get that information. And client B also has to send its SDP to client A for the connection to be complete.

This means there must be whats called a "signaling" server which is a traditional server for both clients to connect to so it can relay the SDP's to each client. This would be a weakness in the captain dirgo network, because if you knock the signaling server out, then it would prevent anyone new from joining the network through webrtc.

However, webtorrent would definitely have the same issue, so I think I can piggyback off of it in order to not need signaling servers of my own. In that case, to kill captain dirgo using this method would kill webtorrent as well, and would be much harder to accomplish.

2022/9/28

Today I spent the whole time examining webtorrent. I had a bad assumption going in, in which I thought webtorrent had its own version of dht to resolve magnet links, so I was very confused.

I wrote a prototype client and did a few experiments with it, so I could see how it worked, and that it did work, so that when I finished analyzed it more deeply I wouldn't come up with nothing.

I ran the following tests:

Seeded a file from desktop on a vpn. Was able to receive file on android chrome outside of vpn. Also did this vice versa even though both were nat'ed.

Seeded a file from desktop under the hotspot of the phone. Cannot receive file on phone. I think them being on the same network was the issue.

So, anyway, after doing some line by line debugging, I decided to turn everything off, DHT, LSD, webseed, to see if webtorrent would still work, and it did.

I had originally thought that webtorrent was using its own version of DHT, and that's how it was transmitting the SDP data so the peers could connect. However, since it was still able to transfer files regardless of it being off, I now believe that it is using a special kind of bittorrent tracker that is acting as a signaling server. This is because the magnet link has a bunch of bittorrent trackers encoded within it, so that would be the way the other peer figures out how to find out the SDP of the first peer.

I know it's rather ridiculous to have to poke through the code like I'm trying to solve a who-done-it mystery, but there is hardly any documentation and webtorrent is pretty huge with tons of options and features. It was really hard to understand how the core worked when there is so many moving parts.

So this leaves me thinking that I might to need to write my own networking code. Honestly, what I've seen so far of Webtorrent looks not so great. It's all plain js, no typescript, and if statements everywhere. I don't want to try and add code to have signaling internally in the network and make sure the peers act in a sane way, and then can also handle malicious peers among all of this stuff. Although it's a mature protocol, I doubt that anyone has really tried to attack webtorrent so I don't know how many flaws could be in there, and I really don't like the idea of trying to maintain it.

Hyperswarm is a little better, but it's geared towards nodejs, the webrtc stuff is kind of tacked on and uses a signaling server. I'll take a look at it further and also see what other things I could do.

2022/9/29

Today I looked at idris and purescript as languages to use for the network backend stuff.

I want to use an advanced language so that I can be relatively sure there aren't any major bugs, and hopefully be easier to maintain.

Purescript is based on Haskell and has some of the features of idris. It looked alright and should work, but I liked idris better, even though its much newer and somewhat experimental. Since it's a dependently typed language, it will allow me to use proofs to verify that it doesn't contain certain types of bugs. Also, I've used idris extensively before, so it help me work faster. The downside is that not too many people know or use it, so I plan to only use it for the backend network code, and leave the front end typescript/svelte.

I got idris compiling and working with javascript, and I should be able to start writing the basis for the network code tomorrow and see how it goes. I'm not ruling out using another library for networking instead, but if I don't find anything good, I will have to write my own.

2022/10/1

Sorry about yesterday, I didn't get a lot done and was quite busy other stuff.

I verified that webtorrent does use the bittorrent-tracker as a signaling server. That means by itself it really wouldn't be very decentralized, and looking at its code I really don't want to use it as a basis due to how messy it is.

I also started fleshing out a plan for the UI. My main goal is to make it as simple as possible to use, and put more advanced features behind an options screen. There isn't much to do for it, just the design of the extension button, the comments page itself, and an options page. I'm thinking of putting a navbar on top for additional features, such as live chat for a particular url.

I watched a video overview of the Elon Musk tweets as it relates to his purchase of twitter. I am not a fanboy of Elon at all, but this business deal has a lot of parrallels with the development of Captain Dirgo. With regard to bots, spam, etc. Captain Dirgo has an advantage here, in that all urls are hashed before they are stored in Captain Dirgo. This means that in order to spam a lot of urls and with the goal of getting a lot people to view it, you would have to go search through a ton of urls and hash each one, hoping for a hit with a lot of recent comments (as a proxy for future view count). Not only that, but you'd probably have to scour the web for what's new, since old urls would probably not get so many hits for your spam. This would be a kind of "natural" mining process that a spammer would have to do, and might make spam not profitable.

Another thing Elon was discussing was putting Twitter on a blockchain and decided that a p2p network wouldn't be able to handle the load. Captain Dirgo has an advantage here because comments on each url are completely separate, which means that sharding is easy. I'm thinking I could do something like DHT to solve that issue, in that each node would handle a certain set of url hashes to hold data for.

I was thinking of the purchase of Twitter itself. Suppose the purchase really did go through and they threw away the original code completely, it seems they are just buying two things. One, the name twitter and the popularity of that platform, and then also the destroying of the old platform, so anyone who used to use it has to find an alternative, and Elon Musk's new twitter would be the obvious choice. So effectively he's buying the chance to get the old twitter audience for $44 billion. It's equivalant to a $44 billion advertising campaign.

So why would Captain Dirgo, which has zero brand recognition and zero initial audience, still have a chance to be a success? The one reason I can come up with is that it's always present and sort of rides on top of the brand recognition of the sites people visit when using it. There is always that curiosity factor of whether someone made a Captain Dirgo comment on a particular url.

My biggest problem that I forsee as of now is how to handle it if the extension is blocked by google/mozilla. Side-loading it as a developer extension in Windows chrome is a big pain in the ass. Even after you go through the complicated process to do it, then everytime you start the browser it will nag you about all side-loaded extensions. Not to mention you'd have to side-load it all over again to get an update. I have a few options here. One is to make the extension a script for tampermonkey. Even though manifest v3 is going to become mandatory in 2023, and that will block Tampermonkey in its current form, I believe Tampermonkey will find a way to keep working. In other words, it makes my problem the same as Tampermonkey's problem. So that if Tampermonkey finds a way to solve it, then I can piggyback on their solution. Also, manifest v3 will block adblockers, so I think there is going to be some shift in the way things are done, which may allow me to keep the extension available and not too difficult to install.

Another option is to create a patch for chromium to prevent it from making manifest v3 mandatory. I will do that if I have to, but I suspect someone else will do it before me.

In any case, I'm going to try and publish it as is and hope the browsers accept it. I don't know why they banned dissenter, but hopefully they will overlook their objections for a completely open source p2p project like this one. (Gab Dissenter had most of its functionality done server side which was not open source, even though the extension was. Literally it just loaded a gab.com url within an iframe within the extension window)

Besides all that, I was looking into https://ceramic.network/ today, which has a seems to have a lot of great ideas, although I don't really understand how it works yet. I'm going to look at it again tomorrow and see what I can use from it, or if I can just plug it in, out of the box.

2022/10/2

I'm trying to design a good protocol. I'd rather not do a coin, because that would entail a blockchain which brings with it a ton of complexity.

Captain Dirgo has a lot of similarities with email, where each url can be thought of an email address. The only question then is who holds the emails and what is their incentive to us this system.

This is my current idea. It would be a DHT based system, where urls are the keys, and comments and other information are the values. The amount of urls that a node is responsible for is configurable by the user.

The incentive model for this network is based on a simplified version of KARMA that Arweave uses: https://arwiki.wiki/#/en/karma (See Basic Mechanism, paragraph 2), where Proof of Work provides the value. I decided that the full Karma protocol, with bank-nodes and currency and transactions was more complex than what I need.

The system centers around CPU efficient hash puzzles with a timestamp attached as a proof that an entity is not acting maliciously or trying to overload the network (ie. Proof of work). I will refer to these solved hash puzzles as PoW.

To begin with, each message may have one or more PoW attached to it. This gives the message value, and allows it to be showed with more promenance in the comments shown to users. When a user upvotes a comment, they perform an PoW that gets attached to message as well. Also, in general the worth of these PoW's decrease over time in terms of scoring (below).

Each node keeps a score attached to every other node that it knows about. Think of it like the tit-for-tat algorithm except for with points instead of just remembering what your peer did and doing the same thing again. Each score is determined by each node individually and different nodes can have different opinions on a particular third node (eg. where *a*,*b* and *c* are nodes in the network, *a* scores *c* with *x* points, and *b* scores *c* with *y* points, where *x* and *y* may not be equal).

Nodes can then ask peers to perform operations depending on the score and what the operation is to be performed.

In general, the score is based on how the node is behaving in relation to the peer and what amounts to a portfolio of good deeds that the node has performed recently. You can think of the idea as a account balance, which is equal to the total of all the good things that a node can prove it has done (PoW being one of them) recently. Since reviewing all the data of an particular portfolio would use too much bandwidth and processing time, a peer samples the good deads of a node to determine its own opinion of the nodes score, using a merkle tree with a digest including an index length.

Specifically, to establish a positive score with a peer, a node can do a few things:

1. Allow the peer to sample messages that the node stores. This is done by having the node provide the peer with a merkle tree hash top of the messages it contains and a count. The peer replies with an index, and the node must provide the message and the merkle path to that index (in order to establish proof that the message provided was in the merkle tree at the corresponding index). The PoWs attached to the message provided will contribute to the peer's evaluation of the node's score.
2. Perform operations for the peer that it requests.
3. Be owned by a user account with a high score (see below for scoring users)

To establish a positive score, a user can provide a list a proof of messages (similiarly to item 1 for nodes, above) with PoW's attached or PoW's of upvotes.

In addition, users or nodes can also establish a positive score by completing a random PoW puzzle with an initial nonce setup by the peer evaluating it. The more difficult it is the higher the score (this may not be linear). This allows a new user or node to get started on the network.

I'm also thinking that maybe that a kind of trust web can work as an enhancement to the above. If the network becomes tight enough, so that nodes are all known by each other through x paths of separation, maybe an algorithm could be designed to take advantage of this to provide another way to score unknown nodes. (ex. Node A wants to establish a score of Node C and already has a score for Node B and Node B already has a score for Node C. Node A can then ask Node B for its score of Node C to help with its evaluation.

As far as figuring out the minimum score necessary to perform task, each node will have a queue of requests by peers and rank them by score.

As far as tasks that peers can ask each other in addition to the above, they would be the following:

1. Establish a connection
1. DHT related tasks (find neighbors)
2. Retrieve a list of encrypted messages for a url hash.
3. Provide a message, or upvote to be stored. (I think I will also be able to provide downvotes)

and probably others, I haven't really completely thought out the tasks just yet.

2022/10/3

I didn't get a too much done today, but I'm starting to experiment with websocket bittorrent trackers and I also looked at i2p. i2p is not designed to work within the browser and I don't see it being easy to connect to without proxies which kind of defeats the purpose.

One thing that I didn't talk about yesterday is that for browser-based nodes to connect to each other directly you have to use webrtc, which has some limitations that must be worked around. For two browser-based nodes, to connect to each other, they must both create an SDP and somehow get the SDP transferred across to the other party. These SDP's can only be used for a single connection. So I wouldn't be able to, for example simply have a node publish its SDP somewhere, because I cannot allow multiple peers to try to connect to the same SDP (I'm actually not quite sure what would happen if two or more other peers did try to use a single node's SDP, but given there so many different types of browsers, it would be difficult in general to say).

This seems to be solved already for me, if I just use existing websocket bittorrent trackers. There aren't that many of them, at least according to here: https://github.com/ngosang/trackerslist/blob/master/trackers_all_ws.txt , so I don't want to rely on this solely. So in addition, I'm thinking of making a Captain Dirgo daemon along with the extension that uses an incoming port. Having actual daemon's in the network would open up additional possiblities, such as publishing that daemon using regular bittorrent trackers and Mainline DHT, which would be much more robust.

The only problem here is what is the incentive? I don't believe that many people would be interested in altrustically supporting the network, because what I've seen with most systems like that is that they languish, especially when they are not very well known (maybe Tor as an exception). So, running the daemon would enhance the score of a user associated with it. It would do so, by making anyone who connects to the app use PoW (as I described yesterday) to have it perform services.

2022/10/4

Today I did a lot of work experimenting with actually coding the extensions.

I found for google chrome to use webrtc to make a p2p network would be pretty difficult. I don't even know if I can do it. There is another extension called Tor Snowflake, which allows browsers to act as an proxy to censored websites, which has about the same problem as mine: / https://twitter.com/torproject/status/1562921730472980481?s=20&t=mDtig24fNUsoXt4ZAJpWmw

The big problem with what Google is doing is that in the next version of their extension api, they take away the ability to do almost anything in the background. The threads in v3 live in little sandboxes that can't do anything besides fetch() calls and listening for events. They have no access to the WebRTC API. You aren't even allowed to pass any executable code to them. I tried passing a lambda function to one, and it was simply stripped out.

I also don't have any access to a long lived webpage in them. The best I could do is have the extension pop up a background tab that the user must leave open, and then maybe I could use webrtc through that. 

Even if that doesn't work, I still think I can make Captain Dirgo work with chrome, though. I just have to change it up a bit. My idea is to make the standalone daemon I was talking about yesterday accept https requests. From a user interface perspective, then, it would basically be the same, anyone that wanted to connect to the daemon would have to provide PoW in order to use it. Whoever owned the daemon could then use that PoW to boost their user's standing. Then I'd use a dumbed down simpler extension that could only make https requests to a server to interface with it.

Firefox is still going to support v2 for the forseeable future and won't have an issue, but Chrome, Chromium, Brave, etc. take over 65% of the marketshare. So what I think I will do is make the simple https version of the extension first with the standalone app, and then come out with a manifest v2 version for Firefox. The manifest v2 version will help strengthen the Captain Dirgo network, and if Google finally backs down and makes v3 less restrictive, then I can update the Chromium version then as well.

This should be the fastest way I can get Captain Dirgo to market, anyway. Also, this simpler version of the extension will allow me to target other browsers as well, so that I shouldn't have much of a problem getting it to work on mobile, either.

2022/10/5

Today I looked in polygon to see if it could help. I think the only way it would work is to make it some sort of paid network where you pay a small fee to write a comment. I don't think that is a good way to go.

I also did a lot of thinking about the Captain Dirgo network itself, below. I apologize that it is so freeform and disorganized, but I really explored a lot of topics so it was hard to put these in a structured form:

If I use tokens as a reward for being a server, how do I even know if the server is properly relaying requests? IOW, how can the contract on polygon matic know whether a user evaluated the server as deserving of coins.

I had another idea that the blockchain contract could accept PoW, but PoW by itself doesn't prove anything. It would be pretty easy to write a program that did a lot of PoW but didn't do anything to support the Captain Dirgo network, so I think thats out.

One other thought I had, how can we prevent a node from cheating by providing its portfolio of proofs merkle tree with duplicate PoW entries? I believe we can do this by just requiring that the entries are sorted. If the entity requiring a proof samples enough merkle paths, it would easily be able to tell if a significant number of entries were duplicated.

I went over the design again and found a flaw. Just because an entity has a lot of proof of work that it did, and a lot of userids associated with it, doesn't mean it didn't just generate the PoW and fake the user ids. Doing so would give it a license to spam if the honest nodes were programmed to just accept it.

So each node needs to keep a network of legitimate peers and users. The only way that PoW can be accepted then is if the sample contains entites that it knows about (ex. has seen the user id/server id in message logs, etc.)

Let's think about PoW. Say a spammer wants to send a million messages. If we force everyone to use 10s of cpu work per message, thats 115 days of work for the spammer, if the machine has roughly the same power. Not too bad to reduce spam, I think.

Lets try and simplify to the most basic logic that I can do while still making it a resilient network:

1. Every action, voting up or down / messages / retrieving data from a server require a PoW. The PoW has to be unique to that action, so for example, an upvote PoW would be seeded from the text of the message, user making it, and timestamp, a message would be seed from the text, user writing it, and the timestamp. Any action that has an identical PoW result is discarded.
2. When displaying messages received from a server node, the PoW is verified. When a message is stored, the server node verifies it.
3. Servers node are in a DHT network
4. Messages are stored as merkle trees within the server nodes. When two server nodes talk to each other, if one doesn't have a merkle tree root or an ancestor of it for a url, it is blocked as being a suspected cheater for a period of time.
5. Messages are also stored as merkle trees by browser nodes, that do the same cheater logic as in 4. (In fact server nodes and browser nodes are almost the same, but browser nodes can display messages for a url.)
6. State can be stored in the merkle nodes. Receiver can ask to sample a select set of paths to verify state is mostly correct.
7. When a browser node talks to various servers, and comes across user ids it stores as having met them. Eventually it will get around to see most of the servers, and user ids and be able to give the user a flair if they own a server, as well as promote messages appropriately.

I'm also wondering about whether I need to use a cpu-friendly algorithm for Proof of Work or just a generic one. If PoW always requires a seed from the other party, it seems that running a gpu program as a one off would be pretty slow. Although a dedicated spammer could queue up a bunch of requests and then run then all at once.

Anyway, there is a cpu friendly algorithm called yespower which might be a good thing to try out.

I also found this which is a prebuilt Proof of Work for the browser:

https://pixelspark.github.io/webpow/pow.html

And this which is a nice gui for the thing:

https://git.sequentialread.com/forest/pow-captcha

I also thought of way to use webrtc with the new chrome extension api (which is extremely limiting). It revolves around the idea that just having an SDP of a peer that is delegated to a particular node is the same as having an active one-way connection. The SDP can be saved in local storage as a string and when needed, a webrtc can be created using it. During the active connection, another SDP pair can be exchanged so that either peer can recreate a connection the next time they want to talk to each other.

The way this would work in the extension, is that it would insert an iframe into every page the user visited, which would be manifest itself as a side bar, that could be opened and closed. When opened, it would activate the webrtc connection through the document itself.

I was also thinking that the server node and the browser node are nearly the same thing. The only difference is that the browser node only stores certain message logs (the ones the user visited and clicked the captain dirgo sidebar on), while the server node stores message logs from its designated hashed urls according to where it is in the DHT network.

So, they both should use the same core architecture. I want to do this in Idris so I can use its typing system to eliminate a lot of possible bugs.

I know that it'll need:

MyUser - user info
User - user info
KnownUserIds - set of known users along with their activity
Connection - one of TCP, UDP, WebSocket, or WebRTC
ActiveConnections - The connections that are active currently

2022/10/6

Chrome has some more limitations I found out today, in that it has only 2 types of storage that an extension can use, local storage which is saved to disk and rather slow, and session storage, in which there is only a 1MB limit.

Also, whenever the user goes to a different url, everything that was in memory gets shutdown. Any connections to other peers would also be shutdown. So it means that any chrome browser's connection to the network will be very spotty, and we have to store and load back all state in the local and session storage when the url changes which is pretty bad.

I think that I'm going to work on firefox and the daemon first, and then go back to chrome after that. Hopefully either google will loosen their grip, or someone will find a way around these restrictions, or I'll think of another idea for using them.

So regardless of the context, browser or daemon, the core code functionality will be about the same:

1. Every 5 minutes or so, we need to announce on bittorrent that our node exists. For browser, it will use a websocket bittorent tracker. (We're piggybacking on the bittorrent network here, so captain dirgo can't be targeted this way easily.)
2. We then join up with a double DHT network, one for users and one for urls. 
3. Then we use a state machine to manage all the events and actions.
4. We also have a db cache to store data.

The db for the browser extension might be somewhat difficult. Besides session store, there is IndexedDB, but I heard that its possible to get corrupted when used with an extension. So, I'll have to do some experiments to see which if any will work.

2022/10/7

Today was a short day, just felt tired, and I also had some tax crap to worry about.

I spent a lot of time trying to understand how best to implement a state machine that works between Idris and javascript. Since javascript uses events there needs be a global object that represents the state.

In the example that I looked at, here: https://medium.com/the-web-tub/idris-state-machines-in-javascript-apps-b969e2cb6ed2 , the author is reading the state from js into idris, processing it in idris and then writing it back to js for every event.

I don't think that idea is a good one, since not only is it quite slow, but it also means that for a complicated state object, you'd need to have a lot of machinery for reading and writing, and then anytime that state object is updated, you'd have to update the machinery along with it. There are libraries for creating generics (which would allow the compiler to generate code to automatic read and write to js) but I didn't want to make the building of the node so complex to depend on additional libraries if I don't have to.

Besides which, it seems to me that simply just storing the ptr to idris data into a global var in js would suffice, and I don't see why explicit read/write calls are any safer or give any advantage to this at all. I'm not entirely positive this is possible but it's hard to image it would not be.

Other than that, the concept the author was using is quite good. Basically you have a state datatype and a "command" datatype. In the "command" datatype, you have within the type the value of the prior state and a function that returns the next state given an argument. Then you define instances for only valid translations of state. If you're not familar with dependent types sorry if that is a little hard to understand.

So, for example, say you got an event back from opening a port. You would have to have an instance in the "command" datatype where the prior state is what it should look like just before opening the port. Then the function that returns the next state would take the javascript event response and if the response was the port opening was successful, the next state would have be one that indicated the port was opened. If the response was that opening the port was unsuccessful, the resulting state would have indicate that the port opening failed. All of this would be done at compile time.

What this allows you to do is avoid any bugs where you forget to check the state is in a proper state before altering it or put it into an incorrect state after the event is finished. In the above example, the compiler would produce an error, if you forgot to check in the port was opened already, or if a port was never requested to be opened.

This functionality is why I really think a dependently typed language like Idris is the way to go for this project. With all the different types of connections, messages and also the different entities that will use this state engine (chrome/firefox/daemon), it creates a huge explosion of different state conditions.

Without having the compiler help make sure that the states are correct, it would be very difficult to avoid having a lot of bugs.

2022/10/8

* Verified that I can pass objects from Idris to Javascript and back again. Idris objects are actually just javascript objects so it's quite easy.
* Started a new project for the Captain Dirgo daemon. It will also be where all the common code is stored for handling the network.
* Got typescript working with nodejs. Also got the vscode debugger to run with typescript.
* Started working with create-bittorrent, and parse-bittorrent libraries. I was able to create a torrent object, but the magnet id is always the same regardless of the contents.
* Started working with commander which is an node command line parser. It's pretty slick, I must say.

I decided to work on the daemon first, since the network code is the biggest unknown right now. It's also shared between the browser and the daemon, and by doing the daemon first makes it easier to test.

I'm thinking of allowing captain dirgo to work off just a magnet uri. This way it can be more claudestine. The bittorrent tracker wouldn't be able to tell without connecting, that the network is actually a Captain Dirgo one. Not sure if it would be useful, but maybe in some locked down jurisidictions, such as in Western Europe that would ban it, would have a harder time finding it. Of course everyone would have to know the specific magnet uri to connect.

2022/10/9

Today was a short day. I read through this on Kademlia DHT: https://codethechange.stanford.edu/guides/guide_kademlia.html

I skimmed through the S/Kademlia academic paper, which is a more secure version of Kademlia:  https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.4986&rep=rep1&type=pdf

I also examined Mainline DHT (used by bittorrent magnet links). Mainline which is based on Kademlia and not S/Kademlia. I'm not sure why it hasn't been DOS'ed, maybe because if it was, anyone using it can go back to sharing regular torrent files until the attackers get bored and quit.

This paper here (warning pdf) https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.4986&rep=rep1&type=pdf from 2012, shows that there seems to have been some real world attacks to shape the network and spy on users on Mainline DHT using sybil attacks.

The first real world attacker seems to be from an ISP who wanted to direct clients to share data with each other within the ISP rather than share with peers outside of it (presumeably to cut costs). The second attacker seemed to be trying to gather info what torrents people are looking for on Mainline. It not only produces a lot of sybils to look for the "infohash" (which is a hash of the torrent file), but also then attempts to join the corresponding bittorrent network and download the torrent metadata to see what the torrent actually is. It's a good read if you are interested in that sort of thing. See page 5, starting at "A. Honeypots" for their real-world data collection. 

I think that using S/Kademlia would largely fix the above attack.

2022/10/11

Yesterday I did a lot reading Kademlia and DHT in general. I didn't really have any insights so I didn't write anything yesterday.

Today I did some more reading on decentralized networks here: https://docs.google.com/presentation/d/11qGZlPWu6vEAhA7p3qsQaQtWH7KofEC9dMeBFZ1gYeA/edit#slide=id.g1718cc2bc_08645

I only skimmed through the last part. It was a nice presentation but it didn't cover anything on adversial nodes, so it doesn't apply 100% to Captain Dirgo. It did talk about gossip protocol which seemed interesting but I didn't delve much into it.

I also went through and analyzed the code of the most popular javascript implement on Mainline DHT (the one that bittorrent uses), so I could get a further handle on what is involved in a DHT network. (notes below)

My plan now is to either modify the DHT system itself or rewrite it to make it work with browser nodes using WebRTC and CPU PoW WebWorkers to handle the karma part (where each user gets a vanity score for all their upvotes on their comments, and well as flair, etc.) I'll think more about this tomorrow.

I was also considering a coin. I do worry that anytime real money is involved, the incentives get skewed. My main concern is that rather than use PoW to support the network, and fight spam, people would just use PoW to generate coins for themselves. Since custom programs created to on dedicated hardware would produce much more hashpower than anything I could write in javascript or wasm, it would mean that 99.99% of coins would go to these people and anyone trying to save comments attached to urls would basically get nothing in comparison for their work.

In other words, the worthlessness of just vanity karma that can't be traded keeps the network more stable then if it was actually worth something.

That said, I would like a global state or something to point all that PoW work at. Right now I don't have anything to point the work towards (it doesn't need to be pointed at anything, but might as well not waste it). I think maybe I could use it to have a canonical state for all url mesages, and a user state in general. So maybe that will work.

--

bittorrent-dht:

//start it up and listen
const dht = new DHT()

dht.listen(20000, function () {
  console.log('now listening')
})

//this will state that it found a peer serving the particular infoHash (this can be thought
// of part of the value of the key)
dht.on('peer', function (peer, infoHash, from) {
  console.log('found potential peer ' + peer.host + ':' + peer.port + ' through ' + from.address + ':' + from.port)
})

// find peers for the given torrent info hash. peers get sent as 'peer' events, listened to as above
dht.lookup(parsed.infoHash, <callback when terminated>)

If we take a closer look:
  lookup
    This calls _closest
      _closest
        creates a new table (k-bucket) for the infohash
	calls k-rpc.closest
	whenever it gets a reply it stores it in the table as a cache (note this is not in
	k-rpc so it doesn't mess with k-rpc tables)
        the peers themselves are the values (since this is bittorrent, after all)
	??? do we follow bittorrent here, or do we directly return a result?
      
  announce(infoHash, [port], [callback])
    If there isn't a table for the torrent already, calls lookup, and callsback announce
    after lookup is done.
    Then calls k-rpc.queryAll() with the q field set to 'announce_peer' and the a field with the port (i guess the address is taken from the message)
    The values of the queryAll are from the k-bucket table according to the infohash.

  put(opts, callback) // writes arbritrary payload
    Has a bunch of logic for multiple key signed data
    But otherwise just runs queryAll with the message 'put'

  get // reads arbritrary payload
    calls _closest
      calls k-rpc.closest

  receiving (in bittorrent-dht)
  onquery: //receives a message from a peer
    possible messages:
      ping: just reply with our id
      find_node: calls k-rpc.nodes.closest(). k-rpc.nodes is k-bucket table, so it pull out all of the closest nodes from its cache
      get_peers: if it has a list of peers for the infohash, will return it, otherwise calls nodes.closest for the infohash to get the closest nodes it knows 
      announce_peer: validates token against calculated host token. then adds the peer (no matter how close it is to the infohash)
      get: if it's got the value cached, it gives the value, otherwise it calls nodes.closest() agai, like above
      put: puts the token, does special stuff if mutable (that I don't want to look at now) then responds.
  onnode: //called whenever a node is added by internal code, re-emits it to client
      
  k-rpc closest
    calls k-rpc._closest
      Makes an empty k-bucket table
      calls its subfunction kick() with an empty k-bucket table
        kick will use "nodes" which is the user's own node's k-bucket table
        if not there, it'll use bootstrap to query nodes 
	kick will then query all the nodes in the k-bucket using the k-rpc-socket.query
	that it can and query any it didn't query before (using the 'queried' boolean array to keep track)
	with a callback of the function, afterQuery
      afterQuery gets the response back, if it's bad it calls kick() again
      if its good, it'll add the peer it tried to connect to to its table
      and call the "visit" callback argument if it was passed in
      then call kick again too

2022/10/12

I was thinking about DHT and I'm not quite sure if it's the best network. The main problem is that it is focused on short simple messages to many different peers, which works well for UDP. However, since I want to be able to have browsers be full nodes on the network, I am relegated to using WebRTC, which is a much slower protocol when establishing connections.

I'm not sure what the implications of that are, and whether it will be a problem or not. I think I need to do some experimentation on that.

The other idea is to use freenet's algorithm, which has long standing connections and would be a better fit for WebRTC. I only skimmed freenet's whitepaper, so I need to delve into it more deeply so that I can better make a decision.

Other than that, I came up with an initial roadmap for Captain Dirgo, which is subject to change which is still based on DHT.

The idea is that version 1 would make it necessary to run a daemon along side the browser. Version 2 would allow firefox to not need a daemon anymore, and version 3 should work with chrome and other browsers. 

Below is the breakdown of the roadmap on a rough architectural level:

Version 1:

* node ids are ip and a public key (like bittorent BEP42 and s/kademlia)
* public key is used as a user id
* we only have a DHT network, nothing underneath
* DHT contains:
  - nodeid - address/port (in the future, webrtc, etc.)
  - userdata - a separate key from userid, contains data associated to user, good and bad. Split into 3 separate locations to prevent fuckery. (using hash of nodeid+1, nodeid+2, nodeid+3). T
  - username - the userid associated with a username ... do we really want this?
  - urlstart - keyed of the hashed url, contains block, or a merkle tree root of contained blocks
  - url+block - a merkle tree node of a url (referenced by a higher up url+block or urlstart)
* To write a message, upvote, etc. a put_value is run
* To use must run a daemon app that browser locally connects to
* nodes keep a log of estimated network size (for determining if other nodes are considered trustworthy given their distance to the infohash)
* upvotes give proof of work

Version 2:

* webrtc integration
  - we need websocket support in the daemons
  - we have special messages for SDP, add, claim (and responses confirming or denying the claim)
* firefox support w/o daemon
* chrome still needs daemon

Version 2.1

* chrome support w/o daemon (hopefully)

Version 3:

* Global state, blockchain.

2022/10/13

I was really busy today with other things so I wasn't able to work much.

I finished reading about the freenet architecture. It shares a lot of similarity with Kademlia DHT in that both use xor to determine an abstract "distance" between nodes and keys and store keys in nodes that have a smaller "distance".

Both seem fine, to be honest. I think Kademlia DHT has a slight advantage in that there probably will be less hops on average than freenet, so I think I'll stick with it.

2022/10/14

Another short day, but I did a little work designing the nodeid's. I'm going to use a combination of ip and pubkey. The mainline DHT network has a nodeid length of 20 bytes, but there is no reason that I can't make it longer if necessary.

I was thinking of how the network could be attacked, what would an attacker most likely want to do. Here are my thoughts on this:

1. DoS a user metadata the attacker doesn't like. The attacker could do this by knocking out the nodeids that are handling the user's metadata including his karma, etc. The only real effect here is that no one could read the user's karma for awhile.
2. Spam a url the attacker wants to censor the comments on. They could do this by spamming a bunch of crap comments to it.
3. DoS a url to try to block people from seeing any comments there.

For 1 and 3 I have an idea where the network will use a counter to automatically switch the nodes that are handling a particular user's metadata. So instead of using hash(url) to choose a set of nodes to save the comments for a url, hash(url+n) would be used, where n=0,1,2,3... When a normal user tries to lookup (url+0) and can't get a response, it would automatically then try (url+1). Then it becomes a game of whack-a-mole. No matter how many nodes an attacker tries to DoS, a normal user would simply increment the 'n' value until he gets a response. During the DoS, some comments or userdata could go missing, but new comments/data could be added. and after the attacker gives up, all the old ones would come back.

For 2, each user has a PoW portfolio, as I was talking about before. So if an attacker used an established user with a strong PoW portfolio to spam, he would accumulate downvotes, etc., hurting their portfolio and eliminating his ability to spam more. If the attacker used a bunch of  freshly minted users, the spam comments made using them would be pushed to the bottom anyway, since those users has no karma.

An attacker could also attempt some whole network attacks, but I think that S/Kademlia should handle these pretty well.

2022/10/15

I read the kademlia white paper carefully today. I've previously read articles and blog posts and looked at a dht implementation in javascript previously, but I wanted to get a better idea of what its trying to accomplish and why it looks that way.

I did a little bit of coding on the daemon. Although this is the first thing I'm working on, by no means do I expect most users to use it. The network will need these daemons, though, in order to faciliate communication between nodes.

I also looked at hyperswarm dht. I might end up using it as a layer, but I need to understand it better.

I'm sorry that it's going so slowly, I am very new to all of this, and having to write this soup to write the whole thing from the networking code to the UI in a language that I also don't know very well makes this more than difficult. But as time progresses it will go faster.

My goal is to get something out there as soon as possible, but make sure that it's stable and works well. The first impression of is really important.

2022/10/17

I was exhausted yesterday so I didn't write anything, but I went through the hyperswarm-web more thouroughly. It was really confusing, being written across 4 libraries and after I got to the bottom of it I realized it didn't do what it described. It uses a centralized server for sending topic data rather than any kind of useful peer-to-peer network. This is different than the regular hyperswarm and it means its rather useless for me.

Today I looked through some more webrtc based projects. Here are my notes:

 * https://github.com/daviddias/webrtc-explorer

    incomplete, uses centeralized signaling server for now.

 * https://github.com/tsujio/webrtc-chord

    uses centralized signaling server.

 * https://github.com/ScholarNinja/extension

    uses webrtc-chord above, a chrome extension, might be nice to reference

 * https://github.com/dominictarr/scuttlebutt

    another complicated system that doesn't explain the protocol so I have no idea whether it is
    what I need or not. Looks well tested, though.

 * https://bit.ly/2P7w6cq (Nat_McAleese_Browser_Hosted_P2P_Resource_Discovery.pdf) https://news.ycombinator.com/item?id=19652516

    Also uses a centralized signaling server. Otherwise, what he was building fits quite nicely with what I'm doing.

I know that webrtc networks are possible without a centralized signaling server because webtorrent doesn't use one, and is probably the most actively used one of the bunch. The issue with it is that there is no webrtc dht connection, so all the peers connect through a tracker, and then they all basically connect to each other to share a particular torrent. This won't scale to the level that captain dirgo needs.

I decided to look closer at freenet. Freenet has had a major shift since version 0.7 where it went from just random peers connecting to the idea that people needed to only connect to their friends, and those friends would connect to others and so on. The point is to hide that you are even using freenet. The problem I have is that the cross-section of most people's friends and those that would try out a new social network system is in most cases probably pretty small or non-existant, so I think that whole idea wouldn't really work well.

However, freenet does have a mode where you can connect to random nodes, so I may investigate it further. In addition, the old freenet wasn't built around the "connection to friends" idea, and that may work. The downside is that I'd have to write it myself, because there is no javascript implementation of any freenet version.

From that last item up above, https://bit.ly/2P7w6cq, I learned that chord dht requires a lot less connections than kademlia and there is even further improvement in that regard with s-chord. I'm not familar with chord so I will probably take a look at it tomorrow.

I might also see if I can improve https://github.com/tsujio/webrtc-chord to not need a fucking signaling server. Jesus fucking christ, I've had to mention that so many times in this blog post. I don't know why no one can design a system without one besides the webtorrent guy.

I also don't know if I really like kademlia DHT for this. Besides the fact that it is designed for DHT and requires loads of connections to other nodes being constantly formed and broken (that would be extremely slow for webrtc), it also doesn't allow nodes to have a lot of overlap. Nodes in a k-bucket (basically a binary prefix) only hold the data under that binary prefix and no more. Since most urls won't hold many comments, the data requirements would be extremely small, so idealy they should hold many of them, even outside their prefix. 

I'm starting to think that I should use just a regular https website server and try and convince other people like the owners of poal.co to come on board. I have no idea how I can make this easy for them, since there is no standardized way to integrate something like this. This is why I went down this other p2p path. The problem is that I can't have the p2p network dogshit slow and it also must be stable, and I haven't found a suitable library, yet.

On second thought, the p2p network is better, because even if sites like poal will eventually allow their users to comment directly in captaindirgo, they could setup a daemon. If I make direct connections, then all that code would be redundant when the p2p network is running.

Maybe it would be good though. It would solve a lot of problems, even make mobile a lot easier. I might have to try it.

2022/10/18

I reviewed Chord and S-Chord. S-Chord in particular seems like a much better solution, because the number of connections that need to be held open are much smaller and more static, which makes browser to browser communication much more workable.

I tried to read about Pastry DHT, but it seemed complicated, and had a very large routing table, so I doubt it would work.

I was thinking of how big sites, like poal, 4chan, etc. can work within the network. My current idea is just to make them regular users, in that all of poal would be a single node. In this way, these big websites could easily verify to the extension that the user exists and is logged in by the cookies already stored in the browser. Also, I could add a little icon that shows the user is from a particular website. This would give any big website that signed up free advertising, which would be an incentive to join the network.

I was also thinking about how to allow bigger sites to cache more of the data than individual users to speed up the network. The issue here is that you'd need to be able to prevent a malicious user from pretending to be a big site, and pretend to cache, but actually censor messages. I don't have the details of this worked out, but the main idea is some sort of global state which is just a merkle tree and everything is verified through it using merkle paths.

2022/10/20

Yesterday I did a little work on architecture and also read more about ICE and "trickle ICE" which deal with WebRTC and getting through nats and firewalls.

Today I looked into what's called udp holepunching. This will make running daemons a lot easier in that you won't have to specifically open up a port on your firewall in most cases. 

2022/10/21

I read about udp holepunching, and protocols for opening up ports in the NAT/firewall, (uPNP, PCP, etc.). Seems like using a good library would be the way to go here, and there are several to choose from.

For bootstrapping, I want to take advantage of bittorrent trackers to prevent a single point of failure. There are about 250 of them listed here: https://www.desmos.com/calculator/lrttqfca8b which would do quite nicely.

I'm pretty sure I can get a browser to pretend to be a bittorrent peer and get a valid response. Then I make a fake captain-dirgo torrent infohash, that actually points to the captain-dirgo network, and thats how nodes will find each other.

The only problem is that the only thing a bittorrent tracker will store is an ip and port. So, I will need some people running daemons that can expose a TCP port on the public internet, which I think I can make easier with uPNP et al.

I was concerned about my sampling idea for PoW because I hadn't heard it being done before. So I researched it further, and found this paper on what's called "Proof of Sequential Work". The thing that made me relieved was that they are actually doing the same technique that I was thinking of: https://eprint.iacr.org/2018/183.pdf

You can see this under section 1.2 PoSW Definition under Challenges (this is just another name for what I was calling "sampling")

The other issue I was worried about was what's called "grinding". This is where you take the random seed for a process (or some other variable) and attempt to manipulate it to your advantage. But I proved to myself that grinding this sort of thing would actually take more work than doing it honestly, even if the sample size is as low as 2.

2022/10/22

I was wrong about what I wrote about grinding yesterday. All it takes is to alter the portfolio a little bit, you don't need to recalculate the whole thing. If it was sequential, you could get away with it, because then all the previous work would be wasted whenever you remove something.

I thought a lot about the network today. It's a bit hard to summarize, so below are the raw notes.

I still have more to think through, but I'm getting closer.

--

I don't think proof of sequential work is too bad, though. When a user upvotes a comment, they are giving another user karma, so they need to do a store anyway, so whats wrong with doing a get first?

But if two users upvote at the same time? One gets wasted? That's not good.

What if we had multiple PoSW paths? I don't want to do a whole claim process and return functionality.

Suppose lets say the user themselves are responsible for storing upvotes? Then they can't get upvotes while offline. 

--

I don't think this is necessary at all. First, we are going to do some sort of "what have you done for me lately" type of scheme. Second, we're not talking about a lot of data.

Let's say a user has 10,000 upvotes in the last week.

Then in order to save the entire portfolio, they only need for sha 256, 8 bytes of space for the hash. Of course there will be other numbers, the ts, the signature, whatever the hash was based on, so as a conservative estimate, lets say 80 bytes of space total (which is a lot).
So that would be 80 * 100000 = around 8 meg. Not a ridiculous amount to store.

Then the friendly peer will simply prove samples of that data to whoever asks.

Not big, we don't need all this mathematical crap. We store the entire portfolio up to a
specific limit, "what have you done for me lately".

As far as a global state floating around and being signed, this seems like the easiest thing.
Just use a gossip protocol among existing connections. 

---

This aint going to work.

First, we can't do "what have you done for me lately" without timestamping. Timestamping requires a global state.

I don't know how we can do a global state. Either every peer has to download the entire history of PoW of everyone, or at least the usernames.

Ok, maybe we could do it. Suppose we have a global state that is represented only by a merkle tree root that everyone gossips around.

Now lets say there are two, and a node wants to know which one to use. It picks a random value the same size as a hash on the DHT and does a special get for usernames that are referential from the global state merkle tree.

Then any node that recognizes this merkle tree that is close to the value will return the closest id thats on the merkle tree, so it will have several, and then use the closest. It then verifies work by that user. If the closest user id is statistically improbable, it's considered a fail.

Will that work? Or is it flawed?

Lets say there is a huge number of adversaries in a bot net... ok this is silly, how can there be so many and still have a functioning network anyway?

---

There are some issues with that, even if it works so far. First, we need the merkle tree balanced, or we're going to have some uber long paths. Second, how do we update without a million
nodes talking over each other?

Third, we were talking about an "ordered" merkle tree. What is that? How can a merkle tree be ordered and balanced? There are such things as self balancing merkle trees.

Here is how I think we should do it. We don't want userdata scattered everywhere,
so instead we have a balanced tree up to the user, and then another balanced tree from
the user down.

This way a user can make any change it likes and it won't mess up everything else.

--

We have a kademlia dht of users (and urls).
We also have a global state balanced merkle tree of users and urls.

Then we somehow update the users by changing the global merkle tree, we don't know exactly how this will work.

Then we have the urls, who is some nodes job to update. A particular url could get teemed with data, and the node just has to suck it up? Who is maintaining these url merkle trees?

Do we need the urls in the global state? Or can they just be inside the DHT alone?

---

Here is another problem. Suppose a user has a grudge against another user, and hashes his id then knocks out his node?

Then he can't talk on the network anymore. It's too effective to be ignored.

So only if node id's can't be determined by user id's does this work.
And then enough nodes have to cache each others data to prevent an individuals user data
from being ddos'ed, or an individual url.

--

We may want to scrap this and go with the original idea, of a bunch of different sites all
wanting access to the network, all with their individual users.

They all gossip all data to one another. What about smurfs? Or do we do a web of trust?

And even if we do a web of trust, we need everyone to save everyone elses data. Will they balk at that?

And what do we know about running a site like that, where everone is attacking it. How can we possibly present code that they would want to use?

And then the cost, because captaindirgo would be the first. And it would be attacked the most. How am I going to pay for this ridiculous thing?

There is this here, https://www.theverge.com/2016/10/3/13155072/4chan-struggling-with-hosting-costs
Admittedly they server images, but still. It's says they spent $6000 per month back in 2009.

---

So I still want to see if decentralization can be used.

So we need a secret nodeid for each user. We probably want to hide source data from other entities as well, so a node can't try and figure out where a comment came from.

Then we have to make sure that upvotes can't reveal the nodeid of a user, as well.

So then no badges for running servers. No karma for doing server things.

--

I can't think of any way of encrypting proof of work where it can still be verified but the creator can't tell they created it. So that makes it theoretically possible to find which user owns which daemon node. It would be difficult. You'd have to cache all the user's data. Then upvote enery node you can find and then wait for it to show up in the user's portfolio. 
So basically that means if you are a user and are accepting proof of work for running a server, you can be outed and people who don't like you can find your ip and ddos it or whatever. 
Node ids are only related to ip and not user ids for the same reason.
Also, there is no real way to tell if a user is really running a daemon or not wrt to karma or badges or whatnot. So running a server can boost karma but you can't be given a badge for doing so.
What if you could establish an informal "credit" woth another node? So that way you can ask pow for your server be given to another server and on and on down a chain.
So you'd ask another node for a deposit account. And they give you a user id. Hmm. Maybe too silly to work. You'd daisy chain through tons of nodes.
Maybe you can just gift any karma you receive to any user you want. 

2022/10/24

Yesterday and today I spent a lot of time reading about the Fiat Shamir heuristic. A short description can be found here: https://hackmd.io/@vKfUEAWlRR2Ogaq8nYYknw/rJ9_8WNLE?type=view , but basically the idea is to take an interactive protocol meant to prove a result and convert it into something that is not interactive, which is also called a proof signature. 

I want to use it for the following. Imagine that whenever a user's comment is upvoted, the upvoter also does a Proof of Work calculation. A popular user could collect thousands of upvotes this way, and I want the total PoW collected as a karma score. The karma score would then be used to sort comments, so higher voted users are displayed first.

The issue is that if each user has potentially thousands of PoW proofs, one for each upvote, it'd be too much processing power and take too much time to total up and verify each user's karma for a particular url, considering that maybe 100s of users might be commenting on it.

So my idea was to that all these PoW proofs from the upvotes collected for each user is thrown into a merkle tree. To verify it, the verifier would only ask for a handful of random paths in the merkle tree and if those are correct, he would assume the whole tree was in all probability correct.

The issue with this is that everytime a verifier wants to look up a users karma, they'd have to make that query to test random paths to a node on the network, which would have to hold all that data, and reply with the result. Considering all the views that a comment might receive, that is a lot of requests.

With the Fiat Shamir heuristic, I could turn a merkle tree of thousands or even millions of small PoW units into a relatively small proof signature.

The other thing this helps with, not an issue actually, but an opportunity, is to make it possible to use these small PoW units to secure a global state, since the proof signature of millions of small PoW units can be used in place of a large amount of PoW work, like bitcoin uses.

The way it works is that instead of a verifier asking for several random PoW units to verify, it does the following:

1. the prover instead passes the merkle tree into a hash function, which is used to derive the first pseudo random index to return a result for, 
2. the prover then rehashes the previous hash appended with the previous result to get the 2nd index to verify,
3. and so on for a specified number of items.

This will prove that the whole set of PoW units is valid up to a certain probability.

It does become a little tricky, though, because a cheating prover could make a merkle tree of PoW units where a certain percentage is fake and the PoW not done, thereby inflating the amount of PoW compared to what he actually did. Then he could alter his suspect merkle tree slightly by changing random fake nodes creating a new hash each time. When run through the Fiat Shamir algorithm he could find one that by chance only looked at valid Pow units, and therefore assumed the whole thing is valid.

So, what I did was come up with an equation to show the minimum number of trials the signature should contain to validate that to create a PoW merkle tree with a specific percentage of fake units would take an amount of processing power that is greater or equal to creating an honest PoW merkle tree with 100% real PoW units.

I'm not a mathematician at all, so here is the way I did that, with a very verbose description of how it's supposed to work:

c - total calculations
PoW - on average c calcuations have run (so a target of around 2 * c)
x - avg number of calculations per upvote
s - security we want. 0.9 = 90% have to be valid for a cheating prover to do the same work as if they made a 100% honest PoW tree
p - indexes tested and stored in signature
m - merkletree branch multiplier. It is the number of calculations to make a branch vs a single hash calculation unit of c
lg2 - log base 2

((1/s) ** p) * m * (lg2 (c/x)) >= c + m * (lg2 (c/x))

(1/s) ** p) : number of tries on average that a cheating prover would have to do to create a valid signature for a merkle tree with (100% - s) invalid PoW nodes (or s percent valid PoW nodes). The assumption here is that the cheating prover can make a new tree for free by simply altering his fake nodes.
c/x : total calculations divided by the avg calculations per upvote
lg2 (c/x) : number of paths for a balanced tree
...  = c + m * (lg2 (c/x)) : c is the total calcs for an honest PoW tree, and lg2 (c/x) is for the construction of the branches of the merkle tree. The idea here is that to grind enough to make a cheating tree have a valid signature with s percent of the units valid (and 100%-s invalid ones), they would have to do the same amount or greater work than an honest prover with 'p' merkle paths stored in the signature.

Solve for p in terms of s,c,m, and x
p = (log((c log(2))/(m log(c/x)) + 1))/log(1/s)

Done here:
https://www.wolframalpha.com/input?i2d=true&i=%5C%2840%29%5C%2840%29Divide%5B1%2Cs%5D%5C%2841%29+**+p%5C%2841%29+*+m+*+%5C%2840%29Log%5B2%2CDivide%5Bc%2Cx%5D%5D%5C%2841%29+%3D+c+%2B+m+*+%5C%2840%29Log%5B2%2CDivide%5Bc%2Cx%5D%5D%5C%2841%29%5C%2844%29+solve+for+p

Online graph so I can mess with the security calculation and see how many paths it would take.
https://www.desmos.com/calculator/rawixoyi9q

If we set s to 70% we get around 80 paths.
If we set s to 90% we get around 300 paths.

A sha256 hash is already 32 bytes. And we still need to store the input data, which would be the sha256 of the other path, and the ordering data (we need ordering so a duplicate PoW unit can't be added to the tree twice, maybe that could be the sha itself though), which means 64 bytes, we may need some other things, dunno.
80 paths, considering 64 bytes per node in path and lets say 1000 nodes, which would be a path length of 10 would mean 80 * 10 * 64 = 51k per signature. Not too shabby. Might be a little more, I guess.
80 paths, considering 64 bytes per node in path and 1,000,000 nodes, would be a path length of 20 would mean 80 * 20 * 64 = 102k per signature.

--

As far as global state goes, I can do it, and it'd be kind of a weird system, because karma couldn't be sent/received, only earned. (I'm not sure how I could do that without the same huge multi GB blockchain sizes that the cryptocoins use. I have some ideas though).

I don't see any use for this global state currently, though. Except possibly to increase security of urls and userdata. (ie make it append only).

2022/10/26

Yesterday I looked into how to make the daemon app have a GUI. I'm thinking of using nwjs for this. It basically turns an app into a regular webpage as far as the backend goes. In this way I can share a lot of code from the daemon with the browser extension.

I thought about turning the daemon into a regular browser as well. This would just be another way to use Captain Dirgo, rather than installing an extension. However, it lacks a back button and bookmarks, so I think if I want to add a Captain Dirgo browser I'd be better off forking Brave or something.

Today I looked into how to structure the app. I experimented with Idris and I'm able to make it export functions, so I can call it from js, and it call back to js, too. So that may work well.

I really don't want to program directly in js, it reminds me of the days I use to develop in Java, C, C++, etc. I recently wrote a tax program in Idris and the amount of bugs that I would end up making are so much less.

I also looked into how I can write multiple packages at the same time. I have a browser client, a backend library and a daemon client. It would be better to split these out each into separate modules. The only problem is that I don't want to deal with having to build and import one project that's used by another every time I make a change.

This looks like a good solution for that: https://javascript.plainenglish.io/how-to-create-multi-module-nodejs-applications-9aff2d73dc17

