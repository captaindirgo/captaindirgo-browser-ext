What about p2p model?

Each node on the service would have a private/public key pair

Messages would be sent back and forth. Messages such as:
get_comments -- given a url get comments
get_username -- get a username for a pubkey

each public key has a txn chain regarding operations on it.
such as:
post_comment
upvote
downvote
trust -- trust other user
untrust
each url would be a set of users that posted to it.

2022/09/04

Today, I'm fasting so I'm making an earlier log entry before I lose all my mental energy.

I've been looking at p2p lately. I thought about it, and using a centralized p2p forum to power the thing seems to be going down a rabbithole. First because of what I've seen the stuff that's out there is very bloated, poorly documented, and hard to customize, and since I'm a beginner it makes it even more so. Besides, even if I were to succeed, then so what? All I'd get out of it is a good prototype, but I'd still have to move to somethig more decentralized.

So, I'm shifting gears and going p2p again. Maybe it's obvious, but I realized when trying to understand what a library does, it's best to look at its dependencies first, so if I want to understand hypercore, I look and see it depends on hyperstream and secret-stream, and then look to see that hyperstream depends on hyperstream/dht, etc. Then I just look at dht and secretstream and then go upwards from there.

So, from what I found, there is no way to create a p2p network without any fixed addresses. Even magnet links have bootstrapping nodes hardcoded in the client. So what I plan to do is to piggyback on bittorrent's magnet protocol. That way, in order to knock out captain dirgo through DHT, you'd have to knock out the whole bittorrent infrastructure.

The big elephant in the room then is spam. There are a couple of projects I want to look at that are also based on hypercore that would have to deal with spam, that I want to look at to get ideas. One is https://github.com/Telios-org which is a p2p email service, and the other is https://cabal.chat/ which is a p2p chat network. I found these here: https://dat-ecosystem.org/

My own idea is to use something like hashcash, so that to post a message, you'd have to mine some data akin to a bitcoin miner. This would be as light as possible, for a url with zero comments, or few views, maybe mining wouldn't be needed at all, but one with a lot of comments, you would have to mine for awhile. Also along with that, having trust networks, so that if your user is trusted by another node on the network it could post without mining.  That other node would have rules, such as having to solve a captcha to initialize the node, and other rules like not posting too quickly.

In the end, its up to each individual user about what messages they want to see or not, so when using the extension you could personally dial in how much spam you are willing to tolerate.

2022/09/05

Didn't have a lot of time today.

Read about Mainline DHT. I think it will work nicely for Captain Dirgo, just present it like a regular torrent with the name "captaindirgo". But when clients connect, they would use captain dirgo's protocol rather than bittorrent.

Started working on adding webtorrent/dht to the extension. This will allow the extension to bootstrap from the regular magnet network that bittorrent uses. Should be pretty hard to shut down.

Played around with web-ext so that I can more easily write a extension working in both chromium and firefox.

2022/9/7

Worked on understanding webpack. I know I am dicking around with a lot of tools, but in the end I think the increased productivity will be worth it.

I understand it pretty well, and it beats everything else that I've seen. It works kind of like java compilation, in that when you require one file from another, it will automatically build the dependency.

Also looked into using lbry as a backend. I'm somewhat skeptical of it for my use case, since it seems to be built around serving static files (ie videos and images). Since captain dirgo is going to be dynamic in that comments are added whenever anyone wants to, I don't think it would work well.

I was thinking about the project last night. Below are some notes, but with the caveat that these are only ideas and I'm not committed to implementing them:

 - I think it should have  a blockchain, so there is a consistent state between all peers. But I don't think it needs mining or a coin. It will have lightweight clients and heavyweight clients, and servers (also heavy weights)
 - Heavyweights can read and provide any part of the blockchain
 - Merkle tree backing 
 - Servers compute final state. (This comment voted up/down, blocked, etc.) And can be queried for state.
 - Users can choose to trust the servers they want.
 - So captaindirgo.com would be a server.
 - Hopefully other people will want to be servers. Websites can also be servers. Benefits would be to draw traffic to their website. This would make captaindirgo harder to kill, because you'd have to kill all the servers. (Also a server doesn't have to have a website, it could be spun up by anybody)
 - Servers could also vouch for users, using email verification, captchas, etc. If a user trusts a particular server, comments vouched by that server would float to the top. This way servers can eliminate spam, but not in a way where they are able to censor the network.
 - Servers could also validate keys with user names. So, a user could have a specific icon next to it indicating that a server has vouched for it (much like a twitter blue check, but each server has their own icon and users could choose which ones they trust).
 - Blocked messages can be stored as just their hash, so that an illegal message, such as a dox or call for violence, can be removed from the blockchain by a particular server (this won't prevent other servers from keeping the data, however).

Status Update 2022/9/8

I just today realized that DHT will not work in the browser, feel kind of stupid about it.

Webtorrent, however, is somehow resolving magnet links within the browser, so I need to
parse the source code to figure out it does so. This way, hopefully I can piggyback on
another service to have a more decentralized way of finding captaindirgo servers.

I saw this pretty good discussion on persistent service workers:

https://stackoverflow.com/questions/66618136/persistent-service-worker-in-chrome-extension

With the new service-worker model that chrome has and mozilla is moving to, I don't think
I can create a persistent peer for a hyperswarm network within the extension without
resorting to some hacks. So I wonder what is the point of a p2p network like that.

Other than looking into this p2p stuff, I started setting up a webpack framework for 
captaindirgo-extension.

2022/9/10

While searching for examples, came across a template for browser extensions. It's the only one that I tried that worked with no errors, and has all the features I want. Hopefully it will save me a lot of time: https://github.com/davidnguyen179/web-extension-svelte-boilerplate

I was able to get it compile out of the box and run in both chromium and firefox.

It's using a framework called svelte which seems pretty cool. I took a look at it and it's a lot easier to use than react and vue and should be faster and less bloated because it generates javascript at compile time rather than runtime like react/vue does.

So today, I went through the tutorial on svelte, and I think this is what I'll use for the extension frontend if nothing goes wrong. I think by learning this framework will be the fastest way for me to create a prototype.

I again didn't get to work too much again, unfortunately, due to having to do some errands around the house.

I was thinking about adding the ability to know how many comments were posted for a website even before clicking on the extension, by making the total show up in the extension icon.

One problem is that for privacy purposes, you don't want the extension looking up every url the user goes to against the server. So, only when the click the extension button do you want the client to ask the server about an individual url.

In order to solve this, I came up with an idea. First the extensio would hash the url using a secure hash algorithm, such as SHA-256, then send, lets say, the first 2 bytes of the hash to the server. This would reduce the number of possible urls that match the hash on average to 1/(256*256) = 1/65536. Then, if the resulting list is small enough, the server could just send all the matching urls along with a total comment count for each. If it's not small enough, then that server would just ask for another byte. By doing it this way, there is no way to determine what url the user went to with only 2 bytes of a sha-256 hash. I mean, literally the client would be sending a string such as "3f" to the server, and there is no way to guess what url the user went to from this. I was also thinking of how to integrate with websites that want to sign up easily. Using the plugin, I can open up any url, including the cookies that are active. So all the website would have to do put a page in the logged-in area of the website that signs the brower-extensions pub key for the user, along with the username and maybe an icon or something. If the user is not logged in, the extension can redirect them to a login page, or open it in a popup window or something and try again afterwards. Then the backend servers for captaindirgo would be able to verify that the user had signed into the site.

Although, I need to check if there are more standard ways to do this, because if there are prebuilt plugins that work with the website, it would make the website owner and my life a whole lot easier.

For the backend servers, I think I want to make them completely separate from the frontend servers. So if you run a website, you can allow your userbase to use the captain dirgo plugin by only building out the signing page. The storage of comments would happen automatically. And if you want to help out by running a backend server, you can do that, too, by running a completely different application, that connects to a p2p network for the captain dirgo backend.

Additionally, anyone could run a backend application without running a website if they wanted to, too.

But that's just my thinking at the moment and could change. My first priority is to get a prototype done, so I can get some feedback on the UI.

2022/9/11

Didn't get much sleep last night. Felt a little off because of it I so did some grunt work. I came up with an initial logo, pretty basic, but it'll do for now.
I also setup gpg for email and initial github repository and updated the README. Here it is: https://github.com/captaindirgo/captaindirgo-browser-ext

From now on I'm going to post status updates there as well as here, in order to increase visibility for the extension and hopefully get other people that want to help to notice.

Went through some more of the svelte tutorial. I think I'll wait until tomorrow or even later before I try to start working on a design/implementing the prototype proper.

I was thinking about the design some more. By default all urls that are sent from the extension should be securely hashed, without the original url ever being known by the server. This will again help user privacy, since even if you have access to the server, you can only tell the website users were commenting on if you happen to guess the url, or brute force it. In addition, I can even use the url with a salt appended to encrypt the comments. In this way, in addition to not knowing what the url is, you couldn't even read the comments unless you knew the url beforehand.

The extension will also always remove all the data after the '?' in a url before hashing it and sending it to a server. Anything after the '?' is where all the personal info is supposed to go. But incase a website developer is stupid enough to design their urls like http://foo.com/<userid>/<account>/<password>/<ssn>/<... other personal details, etc. ...>/, then because of the above, regardless if the user comments on it, the url won't be visible to anyone else including the server owner. 

The above is something the original gab dissenter plugin didn't do, BTW. Anytime a user commented the original dissenter sent everything before the '?' in the url to the gab owned server. I guess they did that so they could maintain a list of 'hot' urls where everyone was commenting to show everyone, like the "trending" data on twitter. I could probably do something like that by having a threshold where if, for example, over 50 users commented on a url, then the next time a user commented on it, it would be released in plaintext to the server. But it'd be kind of messy to implement, and there is a security hole because the server could just lie and tell the extension that 50 users commented on it, so they would release the plain text.

I thought of other idea to help prevent spam by a great deal but I'm too tired today so I think wouldn't be able to explain it well, so I will wait until tomorrow.

Thanks for reading.

2022/9/12

Today I spent the majority of my time going through some more of the svelte tutorial. I started at chapter 5 and now am through 10. There are 19 chapters in total, and I think after I've gone through them all I should be able to start hammering out a gui.

I also want to come up with a handwritten mockup of the components, which I will try to work on tomorrow, not sure when I'll finish. I'll upload it here when it's done.

I realized my idea that I didn't explain yesterday to prevent spam wouldn't work, so I won't hash it out here.

2022/9/13

Finished svelete tutorial. I found a basic comments widget in svelte here: https://github.com/loganwoolf/svelte-comments It's a little basic, being developed to win a contest, but that should make it easier to read and modify.

I was thinking of trying to draw out a design, and other documents yesterday, but I don't know if I really need any of this to get started, as long as I have it laid out in my mind.

So I'm just going to get started and start coding and experimenting and iterate from there.

Learned about https://msgpack.org/index.html which I might adopt for communication between nodes.

Started work on getting an options page up for the extension. Figured that will be a good starting point because everything else is going to need to use it to get user preferences.

Learned about jest, which is a testing framework and was already part of the template that I'm using. Will try to make good use of it.

2022/9/14

I learned some UI stuff, flexblox, browser storage, svelte storage.

I wanted to work on an options page for the extension, but I didn't really want to write a page of fake options since I wasn't sure what I needed, so I decided to delve more into the backend so I had an idea of what I would need.

I read the bittorrent spec and the hypercore whitepaper. Strangely bittorrent seemed more
advanced as far as preventing dos, even though its much older. It has choking to prevent peers from asking for too much data too quickly, and a draft proposal called "Canonical Peer Priority" which helps spread out the node connections. hypercore has built in encryption, but I didn't find it so useful, since everything is going to be encrypted anyway.

I guess bittorrent has been around a long time and therefore had more time to workout all the flaws.

I also looked more into browser based implementations of hypercore and bittorent. There was RangerMauve's implementation of hypercore, which just seems to route all data through a proxy, as far as I could tell reading the source code. The documentation wasn't very clear. This would mean it's basically useless as a p2p network.

I also came across https://github.com/draeder/easypeers which is short little 200 line project that establishes a webtorrent webrtc network. I was able to get it to work both in the browser and nodejs so it seems like a good place to start.

So, I'm leaning towards the webtorrent protocol. I think I'm going to separate the data out per url. This way, anyone who runs the extension can help the network out, and have a cache of comments for recently visited sites, by storing the comments and other data associated to the urls they visit.

2022/9/15

Here is some thinking about the project that I was doing:

Lets say that the extension derives a private/public keypair based on the url, and the sends the public key to the server.

It then uses the private key to encrypt the comment(if its adding a comment) and sign the entire message that is sent to the network to be stored by server nodes

We can't allow a client to send data willy nilly because they could overwhelm the network with data, and create a ton of messages everywhere, drowning out legitimate conversion.

So we have to have some rules on the server nodes.

We will have a web of trust. So there will be an ultimate trust certificate. It can give trust to other websites, attaching a score from 1 to 0 representing a level of trust. certs can then trust other certs and so on and so on and to determine the final trust level, simply multiple the scores down the path off signed certs.

Then based on this score, the extension can decide whether to display a particular message, the server node can decide whether to accept a message from a peer, etc.

Each user needs to sign into captaindirgo.com. It will have a captcha and a email verifier or something.

Sign in will provide a signed message containing a date, with an expiration and a username.

When users post messages, they provide that signed message along with their message.

Server nodes verify this message. They can then have other rules, like post once a minute or something like that.

---

Today I searched the web for projects to help me get a initial website for signing in working. The idea here is to give the extension some sort of server to start the login process. 

I looked across a lot of different examples. It seemed that every one referenced completely different frameworks and helper libraries from each other. Everything is so fragmented and it's really painful to have to learn another library/framework/web server everytime I want to add something to this project.

I found https://github.com/shinokada/svelte-auth seemed pretty good. It doesn't use a lot of different libraries, frameworks, etc. that I don't know about and the demo is pretty advanced. https://svelte-auth-codewithshin.vercel.app/

It turns out that hyperswarm-web only uses the proxy to bridge webrtc networks and regular peers. Also, https://github.com/geut/discovery-swarm-webrtc has a swarm just for web-rtc which I think will work good for this projects.

I also got started with visual studio code. I was using emacs but I think I should try something more modern.

2022/9/16

I didn't have a lot of time to work today. I looked more into sveltekit. The developers decided to remove a piece of major functionality that everyone was using to define a users session and replace it with something else a couple of weeks ago, but all the examples have yet to be updated.

So I had to try and read and understand this one git discussion thread on this new functionality with no examples to work from. I think I can do some experiments and get it to work, but I don't like the way sveltekit is being developed. It's seems ripe to have bugs or easy to misconfigure and get hacked.

I'm kind of tired now, so I'll think about it tomorrow. I think I need to understand it at a deeper level, I suppose, to really get a good opinion on it. Of course I'd be wasting time understanding this new architecture if I do that and it turns out to be crap.

I thought I was getting a leg up on the whole svelte thing by using it in the server, because I'm pretty sure I'm going to use in the extension code. Also having a nice framework to work from opens possiblities for captaindirgo.com to have more functionality than if I use something simpler. So, we'll see.

2022/9/17

I'm still playing around with svelte. There is a user session manager that is stateless on the server side. It works by having the server encrypt a token containing session information to the client which then returns it back as a cookie. Seemed kind of cool and I thought I might use it simply because it shows some working encryption using nodejs which should help me out when I get to the other pieces. It's here: https://github.com/pixelmund/svelte-kit-cookie-session

I'm trying to get a clearer picture of the separation of client and server within the system. It seems that the main workflow is that for each page the browser loads some preferably static page with javascript and then uses a separate fetch to retrieve dynamic information. At first I thought that was not a great idea, because it involves two round trips, but it makes sense from a certain viewpoint. If you create a page with a nice "loading..." mode and fetch the data afterwards with javascript, it would seem faster to the user, because something gets displayed right away.

I'm not planning on using captaindirgo.com for much, just an overview of the extension, how to download it, and a page to register and log-in (this will be a proof-of-concept, mainly, because I plan on integrating with other websites afterwards, using oauth or something like that) but it seemed like a logical place to start, because the extension will have to point to something.

2022/9/18

I read more about svelte and sveltekit and I think I have a better handle on it. The one thing the documentation is missing, as far as I am concerned, is a flow of how the processing works.

Off the top of my head, there is a hooks.server.js, a hooks.client.js, a layout.server.js, a layout.js, a page.js and a page.svelte file, and I believe that's everything that is run during a request, but I could be wrong.

There is also a server route that gets called for api requests, initiated by the client. There is a pattern I guess, where layout.server.js, and page.js loads any data from the database or whatever is needed on the server side, and places it in a js object to be automatically serialized and sent to the client.

This all happens at when the page loads. Then, afterwards the client can issue fetch requests and get back serialized objects from json.

That's how I understand it to work. They've recently changed a lot and the examples are not up to date and the documentation is terse. But the framework is pretty good, it seems from my first look at it.

I don't think I'm going to waste my time looking at any more outdated examples, and just try to mess around and get a site up. I'm pretty confident I can make good progress from here on out.

2022/9/19

I worked on getting the website up and working. Didn't get too far in it, but got the nav bar working and an initial main page. I still need to do a login, signup and download page.

I looked at some different ways to deploy the website. I want to get a prototype up, so I hopefully can get some feedback on the design.

I was thinking about the extension itself. If Google and Mozilla don't accept it, it is somewhat difficult to install, involving multiple steps, and going into developer mode. And on windows using chrome, it will nag the user everytime they startup to disable any sideloaded extensions.

However, if I make a version of the extension to work using Tampermonkey, that would sidestep all of that. Tampermonkey is an extension that can run arbritrary scripts to alter web pages. So the user would just have to install tampermonkey, and then install a Captain Dirgo script rather than an extension which should be a lot easier.

2022/9/20

Got some more experience with Typescript. Made a couple of interfaces for registration and logging in.

Worked a little on the login page, and I got it to display and basically work.

I also looked into oauth and from first glance it doesn't look like it will work. I'm looking for something where the website can sign an authorization that can be verified out of bounds by the p2p network. As far as I can tell, oauth used a blob token.

Just for curiousity and to try and come up with additional better ideas, I learned about jamstack. I thought about maybe using the p2p network as the backend for the website, but my main concern is to mock up what other websites basically use. This way I can model what I need to do to integrate with them.

2022/9/21

A bit of a short day, I had to make a fence to block the dog from eating compost.


Trying to answer a stupid question. Where can I put a var that represents the db so it is accessible by everyone? Without this I'd have to spin up and shutdown a db connection with each request.

The flagship example sidesteps this by calling an external 'api' url.

I guess that would be connection pooling. But the question remains.

Was able to solve it using https://github.com/sveltejs/kit/issues/1538#issuecomment-848002582

Basically create a promise in hooks.server.ts and that promise will run only once, so it
will function like a global var. Then stick it into events.local and it will be accessible in the rest of the app.

But it turns out that the mongodb nodejs driver already implements a connection pool, so I problably won't need a global variable (for that anyway). Nice to know though.

I think from here on out, most status updates are going to be about pretty trivial things. When the rubber hits the road, it's the silly little things that take the most time, especially when you are getting up to speed with a language.


2022/9/22

From experimentation, it looks like any server side "+page.ts" or similar, only runs once
per application.

Note to self, to use require within sveltekit, run:

    import { createRequire } from 'node:module';
    const require = createRequire(import.meta.url);

    // sibling-module.js is a CommonJS module.
    const fs = require('fs');

I wanted to make a config file in the main directory, but I realized that it won't work there, because sveltekit packages the app before runnig it, so I think it has to go into the src/ directory which is a little weird. 

Looked for authentication libraries for handling user sign in:

https://github.com/Dan6erbond/sk-auth
Example app is broken: https://github.com/Dan6erbond/sk-auth/issues/94
Not sure if the rest of it works, since it's based on an older version of sveltekit

https://github.com/pilcrowOnPaper/lucia-sveltekit
A little complicated to setup.
From https://lucia-sveltekit.vercel.app/getting-started
    Apps using Lucia cannot be deployed to edge functions (CloudFlare Workers, Vercel Edge Functions, Netlify Edge Functions) because it has a dependency on Node's crypto module and other Node native APIs.

Tried netlify to see if it applies to me. Netlify seems to work, but since I'm not using the lucia code (only imported it as a dependency), I'm not sure if it's actually going to.

Got an initial site up, sign up does NOT work. Don't expect much, but if you want to see my progress, it here: https://captaindirgo.com

Source code for the web server is now here and is here: https://github.com/captaindirgo/captaindirgo-webapp/

I was thinking about how the network is going to managed. Although, I don't mind being the sole owner of the entire thing, if the ball really starts rolling and it gets big, I don't think it's good to only have me run it. Therefore, I'm thinking of making a sort of trust system.

I wrote about this before, but the idea is that to start off with, there would be one master, me, who would have so many "trust points", lets say 1 million. Then I could give them to others to generate private keys with. In fact captaindirgo.com would obtain a certain limited number of these points, but not all, maybe 1000

In this way, if captaindirgo.com was seized or taken off line, the master key would still hold a majority of points and I'd be able to create a new site, maybe some other way, using tor, etc. to keep the network up.

In addition, eventually I would hope to get other trustworthy people to hold trust points themselves, so that I could give away enough that I am not the single authority ruling the whole system.

This does, of course, sound a lot like a crypto token, and I will have to research more into different crypto networks to see if I can use one to bootstrap this idea, or maybe tweak my idea in order to make the network as resilient as possible.

2022/9/23

Got dotenv working, however I think it's not good enough, because I would have to manually check if all the env names I want are there. So instead, I plan to use io-ts which should automate the error checking for me. io-ts will also help with the server api as well.

However there is a problem in that sveltekit seems to have no designated place for instance level configuration files. I asked them on github discussions, and someone told me to put it under "src" which I find kind of silly. It definately makes things confusing, because usually all data under "src" is checked in, and since this file contains secrets, it definately should not.

Learned about csrf attacks, and I think I can manage it. Sveltekit automatically checks the origin header, so that should fix it, I made a todo.txt file and put an entry to test this so I won't forget.

Got a little farther with lucia. I think it will work fine with netlify. I don't know if that will be the final way I'm going to deploy, but for now it makes for a good testing ground.






