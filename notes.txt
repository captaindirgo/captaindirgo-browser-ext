What about p2p model?

Each node on the service would have a private/public key pair

Messages would be sent back and forth. Messages such as:
get_comments -- given a url get comments
get_username -- get a username for a pubkey

each public key has a txn chain regarding operations on it.
such as:
post_comment
upvote
downvote
trust -- trust other user
untrust
each url would be a set of users that posted to it.

2022/09/04

Today, I'm fasting so I'm making an earlier log entry before I lose all my mental energy.

I've been looking at p2p lately. I thought about it, and using a centralized p2p forum to power the thing seems to be going down a rabbithole. First because of what I've seen the stuff that's out there is very bloated, poorly documented, and hard to customize, and since I'm a beginner it makes it even more so. Besides, even if I were to succeed, then so what? All I'd get out of it is a good prototype, but I'd still have to move to somethig more decentralized.

So, I'm shifting gears and going p2p again. Maybe it's obvious, but I realized when trying to understand what a library does, it's best to look at its dependencies first, so if I want to understand hypercore, I look and see it depends on hyperstream and secret-stream, and then look to see that hyperstream depends on hyperstream/dht, etc. Then I just look at dht and secretstream and then go upwards from there.

So, from what I found, there is no way to create a p2p network without any fixed addresses. Even magnet links have bootstrapping nodes hardcoded in the client. So what I plan to do is to piggyback on bittorrent's magnet protocol. That way, in order to knock out captain dirgo through DHT, you'd have to knock out the whole bittorrent infrastructure.

The big elephant in the room then is spam. There are a couple of projects I want to look at that are also based on hypercore that would have to deal with spam, that I want to look at to get ideas. One is https://github.com/Telios-org which is a p2p email service, and the other is https://cabal.chat/ which is a p2p chat network. I found these here: https://dat-ecosystem.org/

My own idea is to use something like hashcash, so that to post a message, you'd have to mine some data akin to a bitcoin miner. This would be as light as possible, for a url with zero comments, or few views, maybe mining wouldn't be needed at all, but one with a lot of comments, you would have to mine for awhile. Also along with that, having trust networks, so that if your user is trusted by another node on the network it could post without mining.  That other node would have rules, such as having to solve a captcha to initialize the node, and other rules like not posting too quickly.

In the end, its up to each individual user about what messages they want to see or not, so when using the extension you could personally dial in how much spam you are willing to tolerate.

2022/09/05

Didn't have a lot of time today.

Read about Mainline DHT. I think it will work nicely for Captain Dirgo, just present it like a regular torrent with the name "captaindirgo". But when clients connect, they would use captain dirgo's protocol rather than bittorrent.

Started working on adding webtorrent/dht to the extension. This will allow the extension to bootstrap from the regular magnet network that bittorrent uses. Should be pretty hard to shut down.

Played around with web-ext so that I can more easily write a extension working in both chromium and firefox.

2022/9/7

Worked on understanding webpack. I know I am dicking around with a lot of tools, but in the end I think the increased productivity will be worth it.

I understand it pretty well, and it beats everything else that I've seen. It works kind of like java compilation, in that when you require one file from another, it will automatically build the dependency.

Also looked into using lbry as a backend. I'm somewhat skeptical of it for my use case, since it seems to be built around serving static files (ie videos and images). Since captain dirgo is going to be dynamic in that comments are added whenever anyone wants to, I don't think it would work well.

I was thinking about the project last night. Below are some notes, but with the caveat that these are only ideas and I'm not committed to implementing them:

 - I think it should have  a blockchain, so there is a consistent state between all peers. But I don't think it needs mining or a coin. It will have lightweight clients and heavyweight clients, and servers (also heavy weights)
 - Heavyweights can read and provide any part of the blockchain
 - Merkle tree backing 
 - Servers compute final state. (This comment voted up/down, blocked, etc.) And can be queried for state.
 - Users can choose to trust the servers they want.
 - So captaindirgo.com would be a server.
 - Hopefully other people will want to be servers. Websites can also be servers. Benefits would be to draw traffic to their website. This would make captaindirgo harder to kill, because you'd have to kill all the servers. (Also a server doesn't have to have a website, it could be spun up by anybody)
 - Servers could also vouch for users, using email verification, captchas, etc. If a user trusts a particular server, comments vouched by that server would float to the top. This way servers can eliminate spam, but not in a way where they are able to censor the network.
 - Servers could also validate keys with user names. So, a user could have a specific icon next to it indicating that a server has vouched for it (much like a twitter blue check, but each server has their own icon and users could choose which ones they trust).
 - Blocked messages can be stored as just their hash, so that an illegal message, such as a dox or call for violence, can be removed from the blockchain by a particular server (this won't prevent other servers from keeping the data, however).

Status Update 2022/9/8

I just today realized that DHT will not work in the browser, feel kind of stupid about it.

Webtorrent, however, is somehow resolving magnet links within the browser, so I need to
parse the source code to figure out it does so. This way, hopefully I can piggyback on
another service to have a more decentralized way of finding captaindirgo servers.

I saw this pretty good discussion on persistent service workers:

https://stackoverflow.com/questions/66618136/persistent-service-worker-in-chrome-extension

With the new service-worker model that chrome has and mozilla is moving to, I don't think
I can create a persistent peer for a hyperswarm network within the extension without
resorting to some hacks. So I wonder what is the point of a p2p network like that.

Other than looking into this p2p stuff, I started setting up a webpack framework for 
captaindirgo-extension.

2022/9/10

While searching for examples, came across a template for browser extensions. It's the only one that I tried that worked with no errors, and has all the features I want. Hopefully it will save me a lot of time: https://github.com/davidnguyen179/web-extension-svelte-boilerplate

I was able to get it compile out of the box and run in both chromium and firefox.

It's using a framework called svelte which seems pretty cool. I took a look at it and it's a lot easier to use than react and vue and should be faster and less bloated because it generates javascript at compile time rather than runtime like react/vue does.

So today, I went through the tutorial on svelte, and I think this is what I'll use for the extension frontend if nothing goes wrong. I think by learning this framework will be the fastest way for me to create a prototype.

I again didn't get to work too much again, unfortunately, due to having to do some errands around the house.

I was thinking about adding the ability to know how many comments were posted for a website even before clicking on the extension, by making the total show up in the extension icon.

One problem is that for privacy purposes, you don't want the extension looking up every url the user goes to against the server. So, only when the click the extension button do you want the client to ask the server about an individual url.

In order to solve this, I came up with an idea. First the extensio would hash the url using a secure hash algorithm, such as SHA-256, then send, lets say, the first 2 bytes of the hash to the server. This would reduce the number of possible urls that match the hash on average to 1/(256*256) = 1/65536. Then, if the resulting list is small enough, the server could just send all the matching urls along with a total comment count for each. If it's not small enough, then that server would just ask for another byte. By doing it this way, there is no way to determine what url the user went to with only 2 bytes of a sha-256 hash. I mean, literally the client would be sending a string such as "3f" to the server, and there is no way to guess what url the user went to from this. I was also thinking of how to integrate with websites that want to sign up easily. Using the plugin, I can open up any url, including the cookies that are active. So all the website would have to do put a page in the logged-in area of the website that signs the brower-extensions pub key for the user, along with the username and maybe an icon or something. If the user is not logged in, the extension can redirect them to a login page, or open it in a popup window or something and try again afterwards. Then the backend servers for captaindirgo would be able to verify that the user had signed into the site.

Although, I need to check if there are more standard ways to do this, because if there are prebuilt plugins that work with the website, it would make the website owner and my life a whole lot easier.

For the backend servers, I think I want to make them completely separate from the frontend servers. So if you run a website, you can allow your userbase to use the captain dirgo plugin by only building out the signing page. The storage of comments would happen automatically. And if you want to help out by running a backend server, you can do that, too, by running a completely different application, that connects to a p2p network for the captain dirgo backend.

Additionally, anyone could run a backend application without running a website if they wanted to, too.

But that's just my thinking at the moment and could change. My first priority is to get a prototype done, so I can get some feedback on the UI.

2022/9/11

Didn't get much sleep last night. Felt a little off because of it I so did some grunt work. I came up with an initial logo, pretty basic, but it'll do for now.
I also setup gpg for email and initial github repository and updated the README. Here it is: https://github.com/captaindirgo/captaindirgo-browser-ext

From now on I'm going to post status updates there as well as here, in order to increase visibility for the extension and hopefully get other people that want to help to notice.

Went through some more of the svelte tutorial. I think I'll wait until tomorrow or even later before I try to start working on a design/implementing the prototype proper.

I was thinking about the design some more. By default all urls that are sent from the extension should be securely hashed, without the original url ever being known by the server. This will again help user privacy, since even if you have access to the server, you can only tell the website users were commenting on if you happen to guess the url, or brute force it. In addition, I can even use the url with a salt appended to encrypt the comments. In this way, in addition to not knowing what the url is, you couldn't even read the comments unless you knew the url beforehand.

The extension will also always remove all the data after the '?' in a url before hashing it and sending it to a server. Anything after the '?' is where all the personal info is supposed to go. But incase a website developer is stupid enough to design their urls like http://foo.com/<userid>/<account>/<password>/<ssn>/<... other personal details, etc. ...>/, then because of the above, regardless if the user comments on it, the url won't be visible to anyone else including the server owner. 

The above is something the original gab dissenter plugin didn't do, BTW. Anytime a user commented the original dissenter sent everything before the '?' in the url to the gab owned server. I guess they did that so they could maintain a list of 'hot' urls where everyone was commenting to show everyone, like the "trending" data on twitter. I could probably do something like that by having a threshold where if, for example, over 50 users commented on a url, then the next time a user commented on it, it would be released in plaintext to the server. But it'd be kind of messy to implement, and there is a security hole because the server could just lie and tell the extension that 50 users commented on it, so they would release the plain text.

I thought of other idea to help prevent spam by a great deal but I'm too tired today so I think wouldn't be able to explain it well, so I will wait until tomorrow.

Thanks for reading.

2022/9/12

Today I spent the majority of my time going through some more of the svelte tutorial. I started at chapter 5 and now am through 10. There are 19 chapters in total, and I think after I've gone through them all I should be able to start hammering out a gui.

I also want to come up with a handwritten mockup of the components, which I will try to work on tomorrow, not sure when I'll finish. I'll upload it here when it's done.

I realized my idea that I didn't explain yesterday to prevent spam wouldn't work, so I won't hash it out here.

2022/9/13

Finished svelete tutorial. I found a basic comments widget in svelte here: https://github.com/loganwoolf/svelte-comments It's a little basic, being developed to win a contest, but that should make it easier to read and modify.

I was thinking of trying to draw out a design, and other documents yesterday, but I don't know if I really need any of this to get started, as long as I have it laid out in my mind.

So I'm just going to get started and start coding and experimenting and iterate from there.

Learned about https://msgpack.org/index.html which I might adopt for communication between nodes.

Started work on getting an options page up for the extension. Figured that will be a good starting point because everything else is going to need to use it to get user preferences.

Learned about jest, which is a testing framework and was already part of the template that I'm using. Will try to make good use of it.

2022/9/14

I learned some UI stuff, flexblox, browser storage, svelte storage.

I wanted to work on an options page for the extension, but I didn't really want to write a page of fake options since I wasn't sure what I needed, so I decided to delve more into the backend so I had an idea of what I would need.

I read the bittorrent spec and the hypercore whitepaper. Strangely bittorrent seemed more
advanced as far as preventing dos, even though its much older. It has choking to prevent peers from asking for too much data too quickly, and a draft proposal called "Canonical Peer Priority" which helps spread out the node connections. hypercore has built in encryption, but I didn't find it so useful, since everything is going to be encrypted anyway.

I guess bittorrent has been around a long time and therefore had more time to workout all the flaws.

I also looked more into browser based implementations of hypercore and bittorent. There was RangerMauve's implementation of hypercore, which just seems to route all data through a proxy, as far as I could tell reading the source code. The documentation wasn't very clear. This would mean it's basically useless as a p2p network.

I also came across https://github.com/draeder/easypeers which is short little 200 line project that establishes a webtorrent webrtc network. I was able to get it to work both in the browser and nodejs so it seems like a good place to start.

So, I'm leaning towards the webtorrent protocol. I think I'm going to separate the data out per url. This way, anyone who runs the extension can help the network out, and have a cache of comments for recently visited sites, by storing the comments and other data associated to the urls they visit.

2022/9/15

Here is some thinking about the project that I was doing:

Lets say that the extension derives a private/public keypair based on the url, and the sends the public key to the server.

It then uses the private key to encrypt the comment(if its adding a comment) and sign the entire message that is sent to the network to be stored by server nodes

We can't allow a client to send data willy nilly because they could overwhelm the network with data, and create a ton of messages everywhere, drowning out legitimate conversion.

So we have to have some rules on the server nodes.

We will have a web of trust. So there will be an ultimate trust certificate. It can give trust to other websites, attaching a score from 1 to 0 representing a level of trust. certs can then trust other certs and so on and so on and to determine the final trust level, simply multiple the scores down the path off signed certs.

Then based on this score, the extension can decide whether to display a particular message, the server node can decide whether to accept a message from a peer, etc.

Each user needs to sign into captaindirgo.com. It will have a captcha and a email verifier or something.

Sign in will provide a signed message containing a date, with an expiration and a username.

When users post messages, they provide that signed message along with their message.

Server nodes verify this message. They can then have other rules, like post once a minute or something like that.

---

Today I searched the web for projects to help me get a initial website for signing in working. The idea here is to give the extension some sort of server to start the login process. 

I looked across a lot of different examples. It seemed that every one referenced completely different frameworks and helper libraries from each other. Everything is so fragmented and it's really painful to have to learn another library/framework/web server everytime I want to add something to this project.

I found https://github.com/shinokada/svelte-auth seemed pretty good. It doesn't use a lot of different libraries, frameworks, etc. that I don't know about and the demo is pretty advanced. https://svelte-auth-codewithshin.vercel.app/

It turns out that hyperswarm-web only uses the proxy to bridge webrtc networks and regular peers. Also, https://github.com/geut/discovery-swarm-webrtc has a swarm just for web-rtc which I think will work good for this projects.

I also got started with visual studio code. I was using emacs but I think I should try something more modern.

2022/9/16

I didn't have a lot of time to work today. I looked more into sveltekit. The developers decided to remove a piece of major functionality that everyone was using to define a users session and replace it with something else a couple of weeks ago, but all the examples have yet to be updated.

So I had to try and read and understand this one git discussion thread on this new functionality with no examples to work from. I think I can do some experiments and get it to work, but I don't like the way sveltekit is being developed. It's seems ripe to have bugs or easy to misconfigure and get hacked.

I'm kind of tired now, so I'll think about it tomorrow. I think I need to understand it at a deeper level, I suppose, to really get a good opinion on it. Of course I'd be wasting time understanding this new architecture if I do that and it turns out to be crap.

I thought I was getting a leg up on the whole svelte thing by using it in the server, because I'm pretty sure I'm going to use in the extension code. Also having a nice framework to work from opens possiblities for captaindirgo.com to have more functionality than if I use something simpler. So, we'll see.

2022/9/17

I'm still playing around with svelte. There is a user session manager that is stateless on the server side. It works by having the server encrypt a token containing session information to the client which then returns it back as a cookie. Seemed kind of cool and I thought I might use it simply because it shows some working encryption using nodejs which should help me out when I get to the other pieces. It's here: https://github.com/pixelmund/svelte-kit-cookie-session

I'm trying to get a clearer picture of the separation of client and server within the system. It seems that the main workflow is that for each page the browser loads some preferably static page with javascript and then uses a separate fetch to retrieve dynamic information. At first I thought that was not a great idea, because it involves two round trips, but it makes sense from a certain viewpoint. If you create a page with a nice "loading..." mode and fetch the data afterwards with javascript, it would seem faster to the user, because something gets displayed right away.

I'm not planning on using captaindirgo.com for much, just an overview of the extension, how to download it, and a page to register and log-in (this will be a proof-of-concept, mainly, because I plan on integrating with other websites afterwards, using oauth or something like that) but it seemed like a logical place to start, because the extension will have to point to something.

2022/9/18

I read more about svelte and sveltekit and I think I have a better handle on it. The one thing the documentation is missing, as far as I am concerned, is a flow of how the processing works.

Off the top of my head, there is a hooks.server.js, a hooks.client.js, a layout.server.js, a layout.js, a page.js and a page.svelte file, and I believe that's everything that is run during a request, but I could be wrong.

There is also a server route that gets called for api requests, initiated by the client. There is a pattern I guess, where layout.server.js, and page.js loads any data from the database or whatever is needed on the server side, and places it in a js object to be automatically serialized and sent to the client.

This all happens at when the page loads. Then, afterwards the client can issue fetch requests and get back serialized objects from json.

That's how I understand it to work. They've recently changed a lot and the examples are not up to date and the documentation is terse. But the framework is pretty good, it seems from my first look at it.

I don't think I'm going to waste my time looking at any more outdated examples, and just try to mess around and get a site up. I'm pretty confident I can make good progress from here on out.

2022/9/19

I worked on getting the website up and working. Didn't get too far in it, but got the nav bar working and an initial main page. I still need to do a login, signup and download page.

I looked at some different ways to deploy the website. I want to get a prototype up, so I hopefully can get some feedback on the design.

I was thinking about the extension itself. If Google and Mozilla don't accept it, it is somewhat difficult to install, involving multiple steps, and going into developer mode. And on windows using chrome, it will nag the user everytime they startup to disable any sideloaded extensions.

However, if I make a version of the extension to work using Tampermonkey, that would sidestep all of that. Tampermonkey is an extension that can run arbritrary scripts to alter web pages. So the user would just have to install tampermonkey, and then install a Captain Dirgo script rather than an extension which should be a lot easier.

2022/9/20

Got some more experience with Typescript. Made a couple of interfaces for registration and logging in.

Worked a little on the login page, and I got it to display and basically work.

I also looked into oauth and from first glance it doesn't look like it will work. I'm looking for something where the website can sign an authorization that can be verified out of bounds by the p2p network. As far as I can tell, oauth used a blob token.

Just for curiousity and to try and come up with additional better ideas, I learned about jamstack. I thought about maybe using the p2p network as the backend for the website, but my main concern is to mock up what other websites basically use. This way I can model what I need to do to integrate with them.

2022/9/21

A bit of a short day, I had to make a fence to block the dog from eating compost.


Trying to answer a stupid question. Where can I put a var that represents the db so it is accessible by everyone? Without this I'd have to spin up and shutdown a db connection with each request.

The flagship example sidesteps this by calling an external 'api' url.

I guess that would be connection pooling. But the question remains.

Was able to solve it using https://github.com/sveltejs/kit/issues/1538#issuecomment-848002582

Basically create a promise in hooks.server.ts and that promise will run only once, so it
will function like a global var. Then stick it into events.local and it will be accessible in the rest of the app.

But it turns out that the mongodb nodejs driver already implements a connection pool, so I problably won't need a global variable (for that anyway). Nice to know though.

I think from here on out, most status updates are going to be about pretty trivial things. When the rubber hits the road, it's the silly little things that take the most time, especially when you are getting up to speed with a language.


2022/9/22

From experimentation, it looks like any server side "+page.ts" or similar, only runs once
per application.

Note to self, to use require within sveltekit, run:

    import { createRequire } from 'node:module';
    const require = createRequire(import.meta.url);

    // sibling-module.js is a CommonJS module.
    const fs = require('fs');

I wanted to make a config file in the main directory, but I realized that it won't work there, because sveltekit packages the app before runnig it, so I think it has to go into the src/ directory which is a little weird. 

Looked for authentication libraries for handling user sign in:

https://github.com/Dan6erbond/sk-auth
Example app is broken: https://github.com/Dan6erbond/sk-auth/issues/94
Not sure if the rest of it works, since it's based on an older version of sveltekit

https://github.com/pilcrowOnPaper/lucia-sveltekit
A little complicated to setup.
From https://lucia-sveltekit.vercel.app/getting-started
    Apps using Lucia cannot be deployed to edge functions (CloudFlare Workers, Vercel Edge Functions, Netlify Edge Functions) because it has a dependency on Node's crypto module and other Node native APIs.

Tried netlify to see if it applies to me. Netlify seems to work, but since I'm not using the lucia code (only imported it as a dependency), I'm not sure if it's actually going to.

Got an initial site up, sign up does NOT work. Don't expect much, but if you want to see my progress, it here: https://captaindirgo.com

Source code for the web server is now here and is here: https://github.com/captaindirgo/captaindirgo-webapp/

I was thinking about how the network is going to managed. Although, I don't mind being the sole owner of the entire thing, if the ball really starts rolling and it gets big, I don't think it's good to only have me run it. Therefore, I'm thinking of making a sort of trust system.

I wrote about this before, but the idea is that to start off with, there would be one master, me, who would have so many "trust points", lets say 1 million. Then I could give them to others to generate private keys with. In fact captaindirgo.com would obtain a certain limited number of these points, but not all, maybe 1000

In this way, if captaindirgo.com was seized or taken off line, the master key would still hold a majority of points and I'd be able to create a new site, maybe some other way, using tor, etc. to keep the network up.

In addition, eventually I would hope to get other trustworthy people to hold trust points themselves, so that I could give away enough that I am not the single authority ruling the whole system.

This does, of course, sound a lot like a crypto token, and I will have to research more into different crypto networks to see if I can use one to bootstrap this idea, or maybe tweak my idea in order to make the network as resilient as possible.

2022/9/23

Got dotenv working, however I think it's not good enough, because I would have to manually check if all the env names I want are there. So instead, I plan to use io-ts which should automate the error checking for me. io-ts will also help with the server api as well.

However there is a problem in that sveltekit seems to have no designated place for instance level configuration files. I asked them on github discussions, and someone told me to put it under "src" which I find kind of silly. It definately makes things confusing, because usually all data under "src" is checked in, and since this file contains secrets, it definately should not.

Learned about csrf attacks, and I think I can manage it. Sveltekit automatically checks the origin header, so that should fix it, I made a todo.txt file and put an entry to test this so I won't forget.

Got a little farther with lucia. I think it will work fine with netlify. I don't know if that will be the final way I'm going to deploy, but for now it makes for a good testing ground.

2022/9/25

Sorry, yesterday I did do some work, but I felt exhausted at the end of the day and didn't want to post anything.

I got the lucia example working after some small fixes. I'd like to have applied it as it was to my code but I was following a different example that does things through javascript and not a simple form submit like lucia does. All these variation, and the code is so messy in everything that I look at. I feel like I'm looking at code made by animals. It's like seeing a mouse nest snuggled underneath the hood of an old automobile and the mice have been rewiring things.

I never knew that frontend code had become so messy, each piece eschewing different older standards while holding on other ones.

So anyway, things are slow but progressing. I installed a new library to handle internationalization but I wonder how useful it really is. It separates the web page from the text and then you have these 2+ files to constantly go back and forth from. I kind of think if I just put everything into each page itself, then it would be easier to deal with.

I'm trying to make sure that everything I work on now will give me experience for when I code the plugin, so that I gain a double benefit. 

2022/9/26

I finished signup and added a login page, and although rough, they both work well enough. I now have a capable server that can link up with the extension, so I'm going to shift gears a little bit and work on the extension because that is the most important.

I'm thinking about the architecture and how best to implement it. There seems to be 3 layers of security:

1. At the p2p peer level, a peer can be blocked if it's behaving badly and/or spamming.
2. At the user level, heavyweight peers can refuse to save messages from a user who is spamming data.
3. At the reporting level, heavyweight nodes can remove messages or block users.

As far as #3 goes, this has to be available to prevent doxx attacks. However, I plan to make it so each heavyweight node makes its own decision what to store and what not to. That way a single entity can't block a comment it doesn't like for everyone, but only for users and heavyweight nodes that place enough trust in it.

I was also thinking about using an existing blockchain. Ordinary users who run the extension obviously would not be full nodes on any currently running well known blockchain, because in order to do so, most chains, if not all, require downloading huge gigabytes of data. So the only reason to use one is for the heavyweight nodes on the network. The biggest benefit that I can see is if the network goes down due to an attack, the data will be stored in a well known place that no authority can take down. The end result is that existing messages would be stored and held as long as that blockchain was still running.

I also need to design this web of trust. I have a basic idea of how it should work. Users and heavyweight node operators decide which authorities they trust in the plugin, ex. captaindirgo.com, xyzcorp.com, etc. Then when a message is received, etc. it comes along with a web of trust. captaindirgo trusts site xyzcorp.com with a score of 0.1 and xyzcorp.com trusts user 'foo' with a score of 0.36, so if the user trusts captaindirgo with a score of 1, then the final trust value would be 1 * 0.1 * 0.36 = 0.036.  But I worry about the speed of verifying these signatures, however if there becomes too many. If the user has to verify thousands of signatures to read the comments on a url, it could get very cpu intensive.

2022/9/27

I looked back at hyperswarm-web, discovery-web, etc. but wasn't really getting it. So, I decided to investigate more about WebRTC. I thought it was just some protocol that opened tcp sockets and had a library tacked on for video and audio, etc. But I was very wrong.

First, it uses UDP and second it has a lot of functionality to burrow through firewalls, nat routers, and other restricted network environments. It seems to me that it would make it possible for anyone to run a full node on the captaindirgo network by simply opting-in through the browser extension, or using a simple app, without needing to know how to open up a port in the firewall or anything about uPNP, etc. In otherwords, it would just work, regardless of the network environment (although I'm sure there will be exceptions).

I'm basing all this information on this 1hr 10min video I watched today about it. It's actually a great video, you can see it here: https://yewtu.be/watch?v=8I2axE6j204

So, as far as I understand it, webrtc works using these things called SDP's which are similar to ipaddress and ports. SDP's can be enhanced by contacting an outside public server and receiving back an ICE message. This allows a client to know it's outside ip and faciliate p2p connections even when both clients are behind firewalls, nats, etc. without any user configuration.

There is also a drawback to webrtc in that, suppose client A uses it and gets its own SDP. It still needs to establish some sort of connection to client B without using webrtc in order for client B to get that information. And client B also has to send its SDP to client A for the connection to be complete.

This means there must be whats called a "signaling" server which is a traditional server for both clients to connect to so it can relay the SDP's to each client. This would be a weakness in the captain dirgo network, because if you knock the signaling server out, then it would prevent anyone new from joining the network through webrtc.

However, webtorrent would definitely have the same issue, so I think I can piggyback off of it in order to not need signaling servers of my own. In that case, to kill captain dirgo using this method would kill webtorrent as well, and would be much harder to accomplish.

2022/9/28

Today I spent the whole time examining webtorrent. I had a bad assumption going in, in which I thought webtorrent had its own version of dht to resolve magnet links, so I was very confused.

I wrote a prototype client and did a few experiments with it, so I could see how it worked, and that it did work, so that when I finished analyzed it more deeply I wouldn't come up with nothing.

I ran the following tests:

Seeded a file from desktop on a vpn. Was able to receive file on android chrome outside of vpn. Also did this vice versa even though both were nat'ed.

Seeded a file from desktop under the hotspot of the phone. Cannot receive file on phone. I think them being on the same network was the issue.

So, anyway, after doing some line by line debugging, I decided to turn everything off, DHT, LSD, webseed, to see if webtorrent would still work, and it did.

I had originally thought that webtorrent was using its own version of DHT, and that's how it was transmitting the SDP data so the peers could connect. However, since it was still able to transfer files regardless of it being off, I now believe that it is using a special kind of bittorrent tracker that is acting as a signaling server. This is because the magnet link has a bunch of bittorrent trackers encoded within it, so that would be the way the other peer figures out how to find out the SDP of the first peer.

I know it's rather ridiculous to have to poke through the code like I'm trying to solve a who-done-it mystery, but there is hardly any documentation and webtorrent is pretty huge with tons of options and features. It was really hard to understand how the core worked when there is so many moving parts.

So this leaves me thinking that I might to need to write my own networking code. Honestly, what I've seen so far of Webtorrent looks not so great. It's all plain js, no typescript, and if statements everywhere. I don't want to try and add code to have signaling internally in the network and make sure the peers act in a sane way, and then can also handle malicious peers among all of this stuff. Although it's a mature protocol, I doubt that anyone has really tried to attack webtorrent so I don't know how many flaws could be in there, and I really don't like the idea of trying to maintain it.

Hyperswarm is a little better, but it's geared towards nodejs, the webrtc stuff is kind of tacked on and uses a signaling server. I'll take a look at it further and also see what other things I could do.

2022/9/29

Today I looked at idris and purescript as languages to use for the network backend stuff.

I want to use an advanced language so that I can be relatively sure there aren't any major bugs, and hopefully be easier to maintain.

Purescript is based on Haskell and has some of the features of idris. It looked alright and should work, but I liked idris better, even though its much newer and somewhat experimental. Since it's a dependently typed language, it will allow me to use proofs to verify that it doesn't contain certain types of bugs. Also, I've used idris extensively before, so it help me work faster. The downside is that not too many people know or use it, so I plan to only use it for the backend network code, and leave the front end typescript/svelte.

I got idris compiling and working with javascript, and I should be able to start writing the basis for the network code tomorrow and see how it goes. I'm not ruling out using another library for networking instead, but if I don't find anything good, I will have to write my own.

2022/10/1

Sorry about yesterday, I didn't get a lot done and was quite busy other stuff.

I verified that webtorrent does use the bittorrent-tracker as a signaling server. That means by itself it really wouldn't be very decentralized, and looking at its code I really don't want to use it as a basis due to how messy it is.

I also started fleshing out a plan for the UI. My main goal is to make it as simple as possible to use, and put more advanced features behind an options screen. There isn't much to do for it, just the design of the extension button, the comments page itself, and an options page. I'm thinking of putting a navbar on top for additional features, such as live chat for a particular url.

I watched a video overview of the Elon Musk tweets as it relates to his purchase of twitter. I am not a fanboy of Elon at all, but this business deal has a lot of parrallels with the development of Captain Dirgo. With regard to bots, spam, etc. Captain Dirgo has an advantage here, in that all urls are hashed before they are stored in Captain Dirgo. This means that in order to spam a lot of urls and with the goal of getting a lot people to view it, you would have to go search through a ton of urls and hash each one, hoping for a hit with a lot of recent comments (as a proxy for future view count). Not only that, but you'd probably have to scour the web for what's new, since old urls would probably not get so many hits for your spam. This would be a kind of "natural" mining process that a spammer would have to do, and might make spam not profitable.

Another thing Elon was discussing was putting Twitter on a blockchain and decided that a p2p network wouldn't be able to handle the load. Captain Dirgo has an advantage here because comments on each url are completely separate, which means that sharding is easy. I'm thinking I could do something like DHT to solve that issue, in that each node would handle a certain set of url hashes to hold data for.

I was thinking of the purchase of Twitter itself. Suppose the purchase really did go through and they threw away the original code completely, it seems they are just buying two things. One, the name twitter and the popularity of that platform, and then also the destroying of the old platform, so anyone who used to use it has to find an alternative, and Elon Musk's new twitter would be the obvious choice. So effectively he's buying the chance to get the old twitter audience for $44 billion. It's equivalant to a $44 billion advertising campaign.

So why would Captain Dirgo, which has zero brand recognition and zero initial audience, still have a chance to be a success? The one reason I can come up with is that it's always present and sort of rides on top of the brand recognition of the sites people visit when using it. There is always that curiosity factor of whether someone made a Captain Dirgo comment on a particular url.

My biggest problem that I forsee as of now is how to handle it if the extension is blocked by google/mozilla. Side-loading it as a developer extension in Windows chrome is a big pain in the ass. Even after you go through the complicated process to do it, then everytime you start the browser it will nag you about all side-loaded extensions. Not to mention you'd have to side-load it all over again to get an update. I have a few options here. One is to make the extension a script for tampermonkey. Even though manifest v3 is going to become mandatory in 2023, and that will block Tampermonkey in its current form, I believe Tampermonkey will find a way to keep working. In other words, it makes my problem the same as Tampermonkey's problem. So that if Tampermonkey finds a way to solve it, then I can piggyback on their solution. Also, manifest v3 will block adblockers, so I think there is going to be some shift in the way things are done, which may allow me to keep the extension available and not too difficult to install.

Another option is to create a patch for chromium to prevent it from making manifest v3 mandatory. I will do that if I have to, but I suspect someone else will do it before me.

In any case, I'm going to try and publish it as is and hope the browsers accept it. I don't know why they banned dissenter, but hopefully they will overlook their objections for a completely open source p2p project like this one. (Gab Dissenter had most of its functionality done server side which was not open source, even though the extension was. Literally it just loaded a gab.com url within an iframe within the extension window)

Besides all that, I was looking into https://ceramic.network/ today, which has a seems to have a lot of great ideas, although I don't really understand how it works yet. I'm going to look at it again tomorrow and see what I can use from it, or if I can just plug it in, out of the box.

2022/10/2

I'm trying to design a good protocol. I'd rather not do a coin, because that would entail a blockchain which brings with it a ton of complexity.

Captain Dirgo has a lot of similarities with email, where each url can be thought of an email address. The only question then is who holds the emails and what is their incentive to us this system.

This is my current idea. It would be a DHT based system, where urls are the keys, and comments and other information are the values. The amount of urls that a node is responsible for is configurable by the user.

The incentive model for this network is based on a simplified version of KARMA that Arweave uses: https://arwiki.wiki/#/en/karma (See Basic Mechanism, paragraph 2), where Proof of Work provides the value. I decided that the full Karma protocol, with bank-nodes and currency and transactions was more complex than what I need.

The system centers around CPU efficient hash puzzles with a timestamp attached as a proof that an entity is not acting maliciously or trying to overload the network (ie. Proof of work). I will refer to these solved hash puzzles as PoW.

To begin with, each message may have one or more PoW attached to it. This gives the message value, and allows it to be showed with more promenance in the comments shown to users. When a user upvotes a comment, they perform an PoW that gets attached to message as well. Also, in general the worth of these PoW's decrease over time in terms of scoring (below).

Each node keeps a score attached to every other node that it knows about. Think of it like the tit-for-tat algorithm except for with points instead of just remembering what your peer did and doing the same thing again. Each score is determined by each node individually and different nodes can have different opinions on a particular third node (eg. where *a*,*b* and *c* are nodes in the network, *a* scores *c* with *x* points, and *b* scores *c* with *y* points, where *x* and *y* may not be equal).

Nodes can then ask peers to perform operations depending on the score and what the operation is to be performed.

In general, the score is based on how the node is behaving in relation to the peer and what amounts to a portfolio of good deeds that the node has performed recently. You can think of the idea as a account balance, which is equal to the total of all the good things that a node can prove it has done (PoW being one of them) recently. Since reviewing all the data of an particular portfolio would use too much bandwidth and processing time, a peer samples the good deads of a node to determine its own opinion of the nodes score, using a merkle tree with a digest including an index length.

Specifically, to establish a positive score with a peer, a node can do a few things:

1. Allow the peer to sample messages that the node stores. This is done by having the node provide the peer with a merkle tree hash top of the messages it contains and a count. The peer replies with an index, and the node must provide the message and the merkle path to that index (in order to establish proof that the message provided was in the merkle tree at the corresponding index). The PoWs attached to the message provided will contribute to the peer's evaluation of the node's score.
2. Perform operations for the peer that it requests.
3. Be owned by a user account with a high score (see below for scoring users)

To establish a positive score, a user can provide a list a proof of messages (similiarly to item 1 for nodes, above) with PoW's attached or PoW's of upvotes.

In addition, users or nodes can also establish a positive score by completing a random PoW puzzle with an initial nonce setup by the peer evaluating it. The more difficult it is the higher the score (this may not be linear). This allows a new user or node to get started on the network.

I'm also thinking that maybe that a kind of trust web can work as an enhancement to the above. If the network becomes tight enough, so that nodes are all known by each other through x paths of separation, maybe an algorithm could be designed to take advantage of this to provide another way to score unknown nodes. (ex. Node A wants to establish a score of Node C and already has a score for Node B and Node B already has a score for Node C. Node A can then ask Node B for its score of Node C to help with its evaluation.

As far as figuring out the minimum score necessary to perform task, each node will have a queue of requests by peers and rank them by score.

As far as tasks that peers can ask each other in addition to the above, they would be the following:

1. Establish a connection
1. DHT related tasks (find neighbors)
2. Retrieve a list of encrypted messages for a url hash.
3. Provide a message, or upvote to be stored. (I think I will also be able to provide downvotes)

and probably others, I haven't really completely thought out the tasks just yet.

2022/10/3

I didn't get a too much done today, but I'm starting to experiment with websocket bittorrent trackers and I also looked at i2p. i2p is not designed to work within the browser and I don't see it being easy to connect to without proxies which kind of defeats the purpose.

One thing that I didn't talk about yesterday is that for browser-based nodes to connect to each other directly you have to use webrtc, which has some limitations that must be worked around. For two browser-based nodes, to connect to each other, they must both create an SDP and somehow get the SDP transferred across to the other party. These SDP's can only be used for a single connection. So I wouldn't be able to, for example simply have a node publish its SDP somewhere, because I cannot allow multiple peers to try to connect to the same SDP (I'm actually not quite sure what would happen if two or more other peers did try to use a single node's SDP, but given there so many different types of browsers, it would be difficult in general to say).

This seems to be solved already for me, if I just use existing websocket bittorrent trackers. There aren't that many of them, at least according to here: https://github.com/ngosang/trackerslist/blob/master/trackers_all_ws.txt , so I don't want to rely on this solely. So in addition, I'm thinking of making a Captain Dirgo daemon along with the extension that uses an incoming port. Having actual daemon's in the network would open up additional possiblities, such as publishing that daemon using regular bittorrent trackers and Mainline DHT, which would be much more robust.

The only problem here is what is the incentive? I don't believe that many people would be interested in altrustically supporting the network, because what I've seen with most systems like that is that they languish, especially when they are not very well known (maybe Tor as an exception). So, running the daemon would enhance the score of a user associated with it. It would do so, by making anyone who connects to the app use PoW (as I described yesterday) to have it perform services.

2022/10/5

Today I did a lot of work experimenting with actually coding the extensions.

I found for google chrome to use webrtc to make a p2p network would be pretty difficult. I don't even know if I can do it. There is another extension called Tor Snowflake, which allows browsers to act as an proxy to censored websites, which has about the same problem as mine: / https://twitter.com/torproject/status/1562921730472980481?s=20&t=mDtig24fNUsoXt4ZAJpWmw

The big problem with what Google is doing is that in the next version of their extension api, they take away the ability to do almost anything in the background. The threads in v3 live in little sandboxes that can't do anything besides fetch() calls and listening for events. They have no access to the WebRTC API. You aren't even allowed to pass any executable code to them. I tried passing a lambda function to one, and it was simply stripped out.

I also don't have any access to a long lived webpage in them. The best I could do is have the extension pop up a background tab that the user must leave open, and then maybe I could use webrtc through that. 

Even if that doesn't work, I still think I can make Captain Dirgo work with chrome, though. I just have to change it up a bit. My idea is to make the standalone daemon I was talking about yesterday accept https requests. From a user interface perspective, then, it would basically be the same, anyone that wanted to connect to the daemon would have to provide PoW in order to use it. Whoever owned the daemon could then use that PoW to boost their user's standing. Then I'd use a dumbed down simpler extension that could only make https requests to a server to interface with it.

Firefox is still going to support v2 for the forseeable future and won't have an issue, but Chrome, Chromium, Brave, etc. take over 65% of the marketshare. So what I think I will do is make the simple https version of the extension first with the standalone app, and then come out with a manifest v2 version for Firefox. The manifest v2 version will help strengthen the Captain Dirgo network, and if Google finally backs down and makes v3 less restrictive, then I can update the Chromium version then as well.

This should be the fastest way I can get Captain Dirgo to market, anyway. Also, this simpler version of the extension will allow me to target other browsers as well, so that I shouldn't have much of a problem getting it to work on mobile, either.
